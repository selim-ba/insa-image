{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/selim-ba/insa-image/blob/main/Image-TP4-DenoisingInpainting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f1b45c4-86c0-4fc2-9f93-ec9f53e28d75",
      "metadata": {
        "id": "6f1b45c4-86c0-4fc2-9f93-ec9f53e28d75"
      },
      "source": [
        "# Image denoising, inpainting and super-resolution with neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc53c346-b87e-4d76-9834-ba498a7d0416",
      "metadata": {
        "id": "fc53c346-b87e-4d76-9834-ba498a7d0416"
      },
      "source": [
        "In this session, we will see different methods to improve images quality.  \n",
        "In particular, we will use several architectures to remove noise, fill missing parts, and enhance the resolution of images.  \n",
        "Let's begin with a straightforward toy example using the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f4cdb576-262c-4fd2-a1e4-fb5a9f766e1f",
      "metadata": {
        "id": "f4cdb576-262c-4fd2-a1e4-fb5a9f766e1f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53ff8ae6-a00f-451f-ace3-ec02b5edc4b9",
      "metadata": {
        "id": "53ff8ae6-a00f-451f-ace3-ec02b5edc4b9"
      },
      "outputs": [],
      "source": [
        "train_dataset = FashionMNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset  = FashionMNIST('data', train=False, download=True, transform=transforms.ToTensor())\n",
        "trainloader = DataLoader(train_dataset, batch_size=128, num_workers=2, shuffle=True)\n",
        "testloader = DataLoader(test_dataset, batch_size=128, num_workers=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a sample of images from the FashionMNIST dataset:"
      ],
      "metadata": {
        "id": "7FcJITXDY5fC"
      },
      "id": "7FcJITXDY5fC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a220c0c2-341d-4ec5-b641-0b3472b30b13",
      "metadata": {
        "id": "a220c0c2-341d-4ec5-b641-0b3472b30b13"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_img(x):\n",
        "    img_grid = make_grid(x[:16])\n",
        "    plt.figure(figsize=(20,15))\n",
        "    plt.imshow(img_grid.cpu().permute(1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "x, _ = next(iter(trainloader))\n",
        "plot_img(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afb4847a-0708-4ffc-bbc7-869b555c4d75",
      "metadata": {
        "id": "afb4847a-0708-4ffc-bbc7-869b555c4d75"
      },
      "source": [
        "Most of the methods we will use in these sessions are based on [auto-encoders](https://www.cs.toronto.edu/~hinton/science.pdf).  \n",
        "An Autoencoder is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then reconstructs the input with this latent code (the decoder).  \n",
        "![Source wikipedia](https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png)  \n",
        "Let's define our first auto-encoder architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39aa7e69-9650-4b7c-9c34-4b636b220de9",
      "metadata": {
        "tags": [],
        "id": "39aa7e69-9650-4b7c-9c34-4b636b220de9"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        y = self.decoder(z)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53b4a2e5-847f-4aa9-afc5-be2582d82cae",
      "metadata": {
        "id": "53b4a2e5-847f-4aa9-afc5-be2582d82cae"
      },
      "source": [
        "Verify that the defined architecture, outputs a tensor with the same dimensions as its inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4a30312-7730-4cce-8fea-bbc407f154f3",
      "metadata": {
        "id": "d4a30312-7730-4cce-8fea-bbc407f154f3"
      },
      "outputs": [],
      "source": [
        "model = AutoEncoder()\n",
        "y = model(x)\n",
        "print(f'Input shape: {x.shape}')\n",
        "print(f'Reconstruction shape: {y.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62bff66e-1d3f-4f30-8bca-0e8e783649ba",
      "metadata": {
        "id": "62bff66e-1d3f-4f30-8bca-0e8e783649ba"
      },
      "source": [
        "We will train our auto-encoder to minimize an MSE loss between pixels of the original inputs and the reconstrictions.  \n",
        "Fill the following code to define the training method of our auto-encoder.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2d77626-8a02-4564-b180-40082323ad2d",
      "metadata": {
        "id": "a2d77626-8a02-4564-b180-40082323ad2d"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from statistics import mean\n",
        "\n",
        "def train(net, optimizer, loader, epochs=5, noise_factor=0.3):\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = []\n",
        "        t = tqdm(loader) #for progress bar\n",
        "        for x, _ in t:\n",
        "            x = x.to(device) #set x on GPU or CPU\n",
        "            x_hat = net(x) # Get models reconstruction\n",
        "            loss = criterion(x_hat, x)\n",
        "            running_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f'training loss: {mean(running_loss)}')\n",
        "        plot_img(x[:8]) #plot original image\n",
        "        plot_img(torch.clip(x_hat[:8], 0, 1)) #plot reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f2a6f0d-af24-45f0-b620-614b18c0e606",
      "metadata": {
        "id": "9f2a6f0d-af24-45f0-b620-614b18c0e606"
      },
      "source": [
        "Now let's train our first auto-encoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1863e68f-0f4a-4e78-b695-e47283fee680",
      "metadata": {
        "id": "1863e68f-0f4a-4e78-b695-e47283fee680"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = AutoEncoder().to(device)#set the model on GPU or CPU\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train(model, optimizer, trainloader, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "459af45c-325e-4fdc-b32e-3d7497ad6a9e",
      "metadata": {
        "id": "459af45c-325e-4fdc-b32e-3d7497ad6a9e"
      },
      "source": [
        "## Denoising Auto-encoder:\n",
        "\n",
        "Auto-encoders are a simple and yet powerful way to remove noises.  \n",
        "Fill the following method to add a gaussian noise to our input images and train a denoising auto-encoder to reconstruct images without noise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee8e996-dac9-45b8-93d5-99b6b8cb086b",
      "metadata": {
        "id": "dee8e996-dac9-45b8-93d5-99b6b8cb086b"
      },
      "outputs": [],
      "source": [
        "def add_noise(inputs,noise_factor=0.3):\n",
        "    noisy = inputs+torch.randn_like(inputs) * noise_factor\n",
        "    noisy = torch.clip(noisy,0.,1.)\n",
        "    return noisy\n",
        "\n",
        "def train_denoising(net, optimizer, loader, epochs=5, noise_factor=0.3):\n",
        "    criterion = ...\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = []\n",
        "        t = tqdm(loader)\n",
        "        for x, _ in t:\n",
        "            x = x.to(device)\n",
        "            x_noise = ...\n",
        "            outputs = ...\n",
        "            loss = ...\n",
        "            running_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f'training loss: {mean(running_loss)}')\n",
        "        plot_img(x[:8])\n",
        "        plot_img(x_noise[:8])\n",
        "        plot_img(torch.clip(outputs[:8], 0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "def add_noise(inputs,noise_factor=0.3):\n",
        "    noisy = inputs+torch.randn_like(inputs) * noise_factor\n",
        "    noisy = torch.clip(noisy,0.,1.)\n",
        "    return noisy\n",
        "\n",
        "def train_denoising(net, optimizer, loader, epochs=5, noise_factor=0.3):\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = []\n",
        "        t = tqdm(loader)\n",
        "        for x, _ in t:\n",
        "            x = x.to(device)\n",
        "            x_noise = add_noise(x,noise_factor)\n",
        "            outputs = net(x_noise)\n",
        "            loss = criterion(outputs, x)\n",
        "            running_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f'training loss: {mean(running_loss)}')\n",
        "        plot_img(x[:8])\n",
        "        plot_img(x_noise[:8])\n",
        "        plot_img(torch.clip(outputs[:8], 0, 1))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AH_2vMLXaQnY"
      },
      "id": "AH_2vMLXaQnY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00ab963-48c1-4439-96af-eb30c20b1207",
      "metadata": {
        "id": "b00ab963-48c1-4439-96af-eb30c20b1207"
      },
      "outputs": [],
      "source": [
        "model = AutoEncoder().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train_denoising(model, optimizer, trainloader, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7702dc5-226c-4e06-af7b-f0aac15eb1e4",
      "metadata": {
        "id": "d7702dc5-226c-4e06-af7b-f0aac15eb1e4"
      },
      "source": [
        "## Super-Resolution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6800113-a6f0-4728-81c3-5d9c77df401e",
      "metadata": {
        "id": "d6800113-a6f0-4728-81c3-5d9c77df401e"
      },
      "source": [
        "Following the same principle, we can use auto-encoders for other image processing tasks such as resolution enhancement or image inpainting.  \n",
        "Let's use an auto-encoder to enhance the resolution of an image.  \n",
        "We will generate low-resolution images using a pooling layer and train an auto-encoder to reconstruct the images in the original resolution.  \n",
        "Complete the following code to train a super-resolution auto-encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ca98f5-c0a3-4a33-9196-4f5653f57878",
      "metadata": {
        "id": "e0ca98f5-c0a3-4a33-9196-4f5653f57878"
      },
      "outputs": [],
      "source": [
        "def downscale(x, scale_factor=2, upscale=True):\n",
        "    x_low = torch.nn.MaxPool2d(kernel_size=scale_factor)(x)\n",
        "    if upscale:\n",
        "        x_low = torch.nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True)(x_low)\n",
        "    return x_low"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34324854-085e-4c75-8b14-ada24e7abe91",
      "metadata": {
        "id": "34324854-085e-4c75-8b14-ada24e7abe91"
      },
      "outputs": [],
      "source": [
        "def train_upscale(net, optimizer, loader, epochs=5, scale_factor=2, upscale=True):\n",
        "    ...\n",
        "    ...\n",
        "    plot_img(x[:8])\n",
        "    plot_img(x_low_res[:8])\n",
        "    plot_img(torch.clip(x_hat[:8], 0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "def train_upscale(net, optimizer, loader, epochs=5, scale_factor=2, upscale=True):\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = []\n",
        "        t = tqdm(loader)\n",
        "        for x, _ in t:\n",
        "            x = x.to(device)\n",
        "            x_low_res = downscale(x, scale_factor, upscale)\n",
        "            x_hat = net(x_low_res)\n",
        "            loss = criterion(x_hat, x)\n",
        "            running_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f'training loss: {mean(running_loss)}')\n",
        "        plot_img(x[:8])\n",
        "        plot_img(x_low_res[:8])\n",
        "        plot_img(torch.clip(x_hat[:8], 0, 1))"
      ],
      "metadata": {
        "id": "08f3QLAqa5gF"
      },
      "id": "08f3QLAqa5gF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffb5e010-ce6a-4ad2-a852-618eb3359504",
      "metadata": {
        "id": "ffb5e010-ce6a-4ad2-a852-618eb3359504"
      },
      "outputs": [],
      "source": [
        "model = AutoEncoder().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train_upscale(model, optimizer, trainloader, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b154c571-32c9-4a0c-8c67-ed9b9351c350",
      "metadata": {
        "id": "b154c571-32c9-4a0c-8c67-ed9b9351c350"
      },
      "source": [
        "## Inpainting\n",
        "\n",
        "The following function returns a image with missing parts.  \n",
        "Use it to train an auto-encoder on image inpainting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca2498af-3dc5-4faa-9220-6a8b42496387",
      "metadata": {
        "id": "ca2498af-3dc5-4faa-9220-6a8b42496387"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def random_cutout(imgs, min_cut=15,max_cut=45):\n",
        "    n, c, h, w = imgs.shape\n",
        "    w1 = np.random.randint(min_cut, max_cut, n)\n",
        "    h1 = np.random.randint(min_cut, max_cut, n)\n",
        "\n",
        "    w2 = np.random.randint(min_cut, max_cut, n)\n",
        "    h2 = np.random.randint(min_cut, max_cut, n)\n",
        "\n",
        "    cutouts = torch.empty((n, c, h, w), dtype=imgs.dtype, device=imgs.device)\n",
        "    for i, (img, w11, h11, w22, h22) in enumerate(zip(imgs, w1, h1, w2, h2)):\n",
        "        cut_img = img.clone()\n",
        "        cut_img[:, h11:h11 + h11, w11:w11 + w11] = 0\n",
        "        cut_img[:, h22:h22 + h11, w22:w22 + w22] = 0\n",
        "        cutouts[i] = cut_img\n",
        "    return cutouts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4133de8-e61a-47bf-a823-6d482b9965c6",
      "metadata": {
        "id": "c4133de8-e61a-47bf-a823-6d482b9965c6"
      },
      "outputs": [],
      "source": [
        "def train_inpainting(net, optimizer, loader, epochs=5, min_cut=15,max_cut=45):\n",
        "    ...\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "def train_inpainting(net, optimizer, loader, epochs=5, min_cut=15,max_cut=45):\n",
        "    criterion = torch.nn.L1Loss()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = []\n",
        "        t = tqdm(loader)\n",
        "        for x, _ in t:\n",
        "            x = x.to(device)\n",
        "            x_co = random_cutout(x, min_cut, max_cut)\n",
        "            outputs = net(x_co)\n",
        "            loss = criterion(outputs, x)\n",
        "            running_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f'training loss: {mean(running_loss)}')\n",
        "        plot_img(x[:8])\n",
        "        plot_img(x_co[:8])\n",
        "        plot_img(torch.clip(outputs[:8], 0, 1))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SfwK0tj7btKo"
      },
      "id": "SfwK0tj7btKo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf373083-da70-49db-8d7f-db27c23ac6b9",
      "metadata": {
        "id": "bf373083-da70-49db-8d7f-db27c23ac6b9"
      },
      "outputs": [],
      "source": [
        "odel = AutoEncoder().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train_inpainting(model, optimizer, trainloader, epochs=5, min_cut=1,max_cut=27)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294f78c7-325a-4a28-ad58-59b06da58cc0",
      "metadata": {
        "id": "294f78c7-325a-4a28-ad58-59b06da58cc0"
      },
      "source": [
        "# Faces  \n",
        "Now that we have defined all our training procedures, let's try our methods on a more detailed dataset.  \n",
        "We will use the [labeled faces in the wild dataset](http://vis-www.cs.umass.edu/lfw/),  a public benchmark for face verification containing thousands of faces images.  \n",
        "Run the following commands to download and extract the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz > /dev/null 2>&1\n",
        "!tar zxvf lfw.tgz > /dev/null 2>&1\n",
        "!mkdir data > /dev/null 2>&1\n",
        "!mv lfw data > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "MHZTZsaQUUJm"
      },
      "id": "MHZTZsaQUUJm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6fc621d-a3e7-4c4a-b48c-678bcea7224f",
      "metadata": {
        "id": "a6fc621d-a3e7-4c4a-b48c-678bcea7224f"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets.folder import ImageFolder\n",
        "\n",
        "def get_faces_loader(path, **kwargs):\n",
        "    process = transforms.Compose(\n",
        "      [transforms.Resize((120, 120)), transforms.CenterCrop((64, 64)),transforms.ToTensor()])\n",
        "    dataset = ImageFolder(path, process)\n",
        "    lengths = [12000, 1233]\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, lengths)\n",
        "    return DataLoader(train_set, **kwargs), DataLoader(val_set, **kwargs)\n",
        "\n",
        "trainloader, testloader = get_faces_loader(path='data/lfw',\n",
        "                                    batch_size=128,\n",
        "                                    shuffle=True,\n",
        "                                    num_workers=2)\n",
        "\n",
        "\n",
        "x, y = next(iter(trainloader))\n",
        "plot_img(x[:8])\n",
        "plot_img(add_noise(x[:8]))\n",
        "plot_img(downscale(x[:8]))\n",
        "plot_img(random_cutout(x[:8], 4, 60))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a82a7f4-21dd-4603-9587-71aea11e8d11",
      "metadata": {
        "id": "0a82a7f4-21dd-4603-9587-71aea11e8d11"
      },
      "source": [
        "## Auto-Encoder\n",
        "Compared to the FashionMNIST dataset, the images of this dataset are more diverse and with a higher resolution.  \n",
        "Thus using a simple auto-encoder on this dataset may not be as effective as for FashionMNIST.  \n",
        "Let's try anyway to set a baseline to use a simple auto-encoder directly.  \n",
        "We first need to define a new auto-encoder architecture to deal with RGB images and improve the network's capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "990c2f60-754d-46a5-8d36-443f6317813b",
      "metadata": {
        "tags": [],
        "id": "990c2f60-754d-46a5-8d36-443f6317813b"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=4, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d( 128, 64, 4, 2, 0, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d( 64, 32, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d( 32, 3, 4, 2, 1, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        y = self.decoder(z)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f3a8e3-f712-421e-9dea-2539698db3fe",
      "metadata": {
        "id": "e4f3a8e3-f712-421e-9dea-2539698db3fe"
      },
      "source": [
        "Verify that the defined architecture, outputs a tensor with the same dimensions as its inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fa7b6d2-b0e4-4765-8a60-19d3d0306462",
      "metadata": {
        "id": "9fa7b6d2-b0e4-4765-8a60-19d3d0306462"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "model = AutoEncoder()\n",
        "y = model(x)\n",
        "print(f'Input shape: {x.shape}')\n",
        "print(f'Reconstruction shape: {y.shape}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vaN5ldtVcTq1"
      },
      "id": "vaN5ldtVcTq1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "20870bf2-821b-4238-be48-4414bd80e904",
      "metadata": {
        "id": "20870bf2-821b-4238-be48-4414bd80e904"
      },
      "source": [
        "### Denoising\n",
        "\n",
        "Use this new architecture to train a denoising auto-encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1d361e8-ef6f-4fb1-b4dd-5d9c3df4cbf2",
      "metadata": {
        "tags": [],
        "id": "b1d361e8-ef6f-4fb1-b4dd-5d9c3df4cbf2"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "model = AutoEncoder().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train_denoising(model, optimizer, trainloader, epochs=10)"
      ],
      "metadata": {
        "id": "LpolblOZccmK"
      },
      "id": "LpolblOZccmK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "496ef68a-27c8-462b-a24c-7262a9dea632",
      "metadata": {
        "id": "496ef68a-27c8-462b-a24c-7262a9dea632"
      },
      "source": [
        "### Super-resolution\n",
        "Do the same to enhance the resolution on this new datase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cc1b9fe-49d6-4184-add4-b396330889f4",
      "metadata": {
        "tags": [],
        "id": "2cc1b9fe-49d6-4184-add4-b396330889f4"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "model = AutoEncoder().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train_upscale(model, optimizer, trainloader, epochs=10)"
      ],
      "metadata": {
        "id": "-kP6xIrvcms8"
      },
      "id": "-kP6xIrvcms8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "56b09cb3-d578-47df-b362-484725b742f9",
      "metadata": {
        "id": "56b09cb3-d578-47df-b362-484725b742f9"
      },
      "source": [
        "### Inpainting\n",
        "Train your auto-encoder to fill the missing parts on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a009e9b-bd6c-472d-b7e5-a4cb615aa24f",
      "metadata": {
        "tags": [],
        "id": "1a009e9b-bd6c-472d-b7e5-a4cb615aa24f"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "model = AutoEncoder().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train_inpainting(model, optimizer, trainloader, epochs=10, min_cut=4, max_cut=60)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AxH-xi4Fczme"
      },
      "id": "AxH-xi4Fczme",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0972b5fa-9442-489c-b55a-bf9167f3aff3",
      "metadata": {
        "id": "0972b5fa-9442-489c-b55a-bf9167f3aff3"
      },
      "source": [
        "# Unet\n",
        "The results may seem a little disappointing.  \n",
        "This is normal, we used a very basic technique and a rather light architecture.  \n",
        "A first improvement we can make to our baseline is to use an architecture more adapted to these types of problems.  \n",
        "We will implement a new auto-encoder architecture called U-net.\n",
        "U-net networks are auto-encoders using skips between the encoder and decoder layers.\n",
        "![](https://github.com/wikistat/AI-Frameworks/blob/website/docs/img/AU_UNet.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25818944-9d5c-4066-8292-db1cd07175e2",
      "metadata": {
        "id": "25818944-9d5c-4066-8292-db1cd07175e2"
      },
      "source": [
        "Help yourself with the image below to implement a Unet network using the following template:\n",
        "![](https://drive.google.com/uc?export=view&id=1HDP2DQRAZyjGugBUkPR8svMt4kyF3p4J)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e09945c-ba07-4e72-bb3a-4d6bf9445863",
      "metadata": {
        "id": "5e09945c-ba07-4e72-bb3a-4d6bf9445863"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def double_conv(in_channels, out_channels):\n",
        "    # returns a block compsed of two Convolution layers with ReLU activation function\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "class DownSampleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_block = double_conv(in_channels, out_channels)\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_skip = self.conv_block(x)\n",
        "        x = self.maxpool(x_skip)\n",
        "\n",
        "        return x , x_skip\n",
        "\n",
        "class UpSampleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_block = double_conv(in_channels, out_channels)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "    def forward(self, x, x_skip):\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, x_skip], dim=1)\n",
        "        x = self.conv_block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample_block_1 = ...\n",
        "        self.downsample_block_2 = ...\n",
        "\n",
        "        self.middle_conv_block = ...\n",
        "\n",
        "        self.upsample_block_2 = ...\n",
        "        self.upsample_block_1 = ...\n",
        "\n",
        "        self.last_conv = ...\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, x_skip1 = ...\n",
        "        x, x_skip2 = ...\n",
        "\n",
        "        ...\n",
        "\n",
        "        out = self.last_conv(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def double_conv(in_channels, out_channels):\n",
        "    # returns a block compsed of two Convolution layers with ReLU activation function\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "class DownSampleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_block = double_conv(in_channels, out_channels)\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_skip = self.conv_block(x)\n",
        "        x = self.maxpool(x_skip)\n",
        "\n",
        "        return x , x_skip\n",
        "\n",
        "class UpSampleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_block = double_conv(in_channels, out_channels)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "    def forward(self, x, x_skip):\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, x_skip], dim=1)\n",
        "        x = self.conv_block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample_block_1 = DownSampleBlock(in_channels, 64)\n",
        "        self.downsample_block_2 = DownSampleBlock(64, 128)\n",
        "        self.middle_conv_block = double_conv(128, 256)\n",
        "\n",
        "\n",
        "        self.upsample_block_2 = UpSampleBlock(128 + 256, 128)\n",
        "        self.upsample_block_1 = UpSampleBlock(128 + 64, 64)\n",
        "\n",
        "        self.last_conv = nn.Conv2d(64, out_channels, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, x_skip1 = self.downsample_block_1(x)\n",
        "        x, x_skip2 = self.downsample_block_2(x)\n",
        "\n",
        "        x = self.middle_conv_block(x)\n",
        "\n",
        "        x = self.upsample_block_2(x, x_skip2)\n",
        "        x = self.upsample_block_1(x, x_skip1)\n",
        "\n",
        "        out = self.last_conv(x)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6SfEb5DjdGzV"
      },
      "id": "6SfEb5DjdGzV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b4596a7c-1d6a-4f9e-8a7c-ef916e9fe004",
      "metadata": {
        "id": "b4596a7c-1d6a-4f9e-8a7c-ef916e9fe004"
      },
      "source": [
        "Verify that the defined architecture, outputs a tensor with the same dimensions as its inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1458072-d563-4995-a814-0b7175609892",
      "metadata": {
        "id": "f1458072-d563-4995-a814-0b7175609892"
      },
      "outputs": [],
      "source": [
        "model = UNet()\n",
        "y = model(x)\n",
        "print(f'Input shape: {x.shape}')\n",
        "print(f'Reconstruction shape: {y.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217c7b80-a5f3-404e-baa7-6f4ec06a98a0",
      "metadata": {
        "id": "217c7b80-a5f3-404e-baa7-6f4ec06a98a0"
      },
      "source": [
        "Train a U-Net on one of the previous tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4d6adfe-0693-4dd3-a293-9d346c8cf2ac",
      "metadata": {
        "id": "a4d6adfe-0693-4dd3-a293-9d346c8cf2ac"
      },
      "source": [
        "### Denoising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9054fdd-38b7-4108-b0bf-d547a702d23b",
      "metadata": {
        "tags": [],
        "id": "b9054fdd-38b7-4108-b0bf-d547a702d23b"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "model = UNet().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train_denoising(model, optimizer, trainloader, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-trained models"
      ],
      "metadata": {
        "id": "Vz_ufMNGXLpy"
      },
      "id": "Vz_ufMNGXLpy"
    },
    {
      "cell_type": "code",
      "source": [
        "#The folowing cell will download the weights of pre-trained models.\n",
        "# You can use it to watch the results of a model trained without having to wait for the full training\n",
        "!wget https://github.com/DavidBert/INSA_notebooks/raw/main/weights.zip\n",
        "!unzip weights.zip"
      ],
      "metadata": {
        "id": "qaOO9ZeBUkKm"
      },
      "id": "qaOO9ZeBUkKm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet().to(device)\n",
        "model.load_state_dict(torch.load('models/unet_denoising.pth'))\n",
        "x, _ = next(iter (testloader))\n",
        "x_prime = add_noise(x, 0.1)\n",
        "x_hat = model(x_prime.cuda())\n",
        "plot_img(x[:8])\n",
        "plot_img(x_prime[:8])\n",
        "plot_img(torch.clip(x_hat[:8], 0, 1))"
      ],
      "metadata": {
        "id": "F355GYCuVx0c"
      },
      "id": "F355GYCuVx0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f7e86add-79f1-439f-afbf-aa3509b5b62e",
      "metadata": {
        "id": "f7e86add-79f1-439f-afbf-aa3509b5b62e"
      },
      "source": [
        "### Super-resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a61fead0-c8c0-4653-bdf1-321af72feb6f",
      "metadata": {
        "tags": [],
        "cellView": "form",
        "id": "a61fead0-c8c0-4653-bdf1-321af72feb6f"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "model = UNet().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train_upscale(model, optimizer, trainloader, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Trained model results\n",
        "model = UNet().to(device)\n",
        "model.load_state_dict(torch.load('models/unet_super_resolution.pth'))\n",
        "x, _ = next(iter (testloader))\n",
        "x_prime = downscale(x, 2)\n",
        "x_hat = model(x_prime.cuda())\n",
        "plot_img(x[:8])\n",
        "plot_img(x_prime[:8])\n",
        "plot_img(torch.clip(x_hat[:8], 0, 1))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sOEETu41XDbd"
      },
      "id": "sOEETu41XDbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "df72a1e8-86b7-4fad-b1dd-c2209ace6e50",
      "metadata": {
        "tags": [],
        "id": "df72a1e8-86b7-4fad-b1dd-c2209ace6e50"
      },
      "source": [
        "### Inpainting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baf62495-45ee-497d-8cc5-5d351a90b7f0",
      "metadata": {
        "tags": [],
        "cellView": "form",
        "id": "baf62495-45ee-497d-8cc5-5d351a90b7f0"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "model = UNet().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "train_inpainting(model, optimizer, trainloader, epochs=10, min_cut=4, max_cut=60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Trained model results\n",
        "model = UNet().to(device)\n",
        "model.load_state_dict(torch.load('models/unet_inpainting.pth'))\n",
        "x, _ = next(iter (testloader))\n",
        "x_prime = random_cutout(x, 4, 60)\n",
        "x_hat = model(x_prime.cuda())\n",
        "plot_img(x[:8])\n",
        "plot_img(x_prime[:8])\n",
        "plot_img(torch.clip(x_hat[:8], 0, 1))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Kv02FyzqXuNU"
      },
      "id": "Kv02FyzqXuNU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "04e35c26-d6e1-4c9b-8dc5-8e9241a44188",
      "metadata": {
        "id": "04e35c26-d6e1-4c9b-8dc5-8e9241a44188"
      },
      "source": [
        "# PixelShuffle\n",
        "\n",
        "The [Real-Time Single Image and Video Super-Resolution Using an EfficientSub-Pixel Convolutional Neural Network](https://arxiv.org/pdf/1609.05158.pdf) paper introduced an original and efficient upscaling layer known as __Pixel shuffle layer__.\n",
        "It introduces a sub-pixel convolution taking into account pixels from different channels of the low-resolution feature map to output the upscaled high-resolution outputs.  \n",
        "![](https://i.stack.imgur.com/bMcYK.png)\n",
        "Its principle and intuition is described in detail in [this complementary material](https://arxiv.org/pdf/1609.07009.pdf) provided by the authors.  \n",
        "\n",
        "The Pixel Shuffle layer is already implmented in Pytorch (https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html).\n",
        "\n",
        "The original paper does not use an auto-encoder to produce high-resolution images but only a simple convolutional network with a final Pixel Shuffle layer at the outputs.  \n",
        "For sake of simplicity and to speed up the computation we will also use it a a simple network, but it could be associated to a U-Net without any difficulty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2b696e4-f146-447c-9230-5df38639cc48",
      "metadata": {
        "id": "f2b696e4-f146-447c-9230-5df38639cc48"
      },
      "outputs": [],
      "source": [
        "class SubPixelNetwork(nn.Module):\n",
        "    def __init__(self, upscale_factor):\n",
        "        super(SubPixelNetwork, self).__init__()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv1 = nn.Conv2d(3, 64, (5, 5), (1, 1), (2, 2))\n",
        "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
        "        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
        "        self.conv4 = nn.Conv2d(32, (upscale_factor ** 2)*3, (3, 3), (1, 1), (1, 1))\n",
        "        # Sub-pixel convolution: rearranges elements in a Tensor of shape (*, r^2C, H, W)\n",
        "        # to a tensor of shape (C, rH, rW)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.pixel_shuffle(self.conv4(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0179a890-5b61-49fc-b99f-40116b3cc094",
      "metadata": {
        "id": "0179a890-5b61-49fc-b99f-40116b3cc094"
      },
      "outputs": [],
      "source": [
        "model = SubPixelNetwork(2).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train_upscale(model, optimizer, trainloader, epochs=10, upscale=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Trained model results\n",
        "model = SubPixelNetwork(2).cuda()\n",
        "model.load_state_dict(torch.load('models/pixel_shuffle.pth'))\n",
        "x, _ = next(iter (testloader))\n",
        "x_prime = downscale(x, 2, False)\n",
        "x_hat = model(x_prime.cuda())\n",
        "plot_img(x[:8])\n",
        "plot_img(x_prime[:8])\n",
        "plot_img(torch.clip(x_hat[:8], 0, 1))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lmRrGKp5X563"
      },
      "id": "lmRrGKp5X563",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c77c5eb5-7f53-4210-9063-cae97c9789e6",
      "metadata": {
        "id": "c77c5eb5-7f53-4210-9063-cae97c9789e6"
      },
      "source": [
        "# Partial Convolutions\n",
        "\n",
        "The [Image Inpainting for Irregular Holes using Partial Convolutions paper](https://arxiv.org/abs/1804.07723) introduced an original convolutional layer to deal with the missing parts of image inpainting problems.   \n",
        "They proposed the use of __partial convolutions__, where the convolution is masked and renormalized to be conditioned on only valid pixels.  \n",
        "Partial convolution layers will thus partially apply a convolution on the inputs according to a binary mask image representing the valid and missing pixels.  \n",
        "\n",
        "The partial convolution is computed according to the following formula:  \n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRSitPJGC7FpseNJC4h5WeEnQadbQOwFI1Bag&usqp=CAU)  \n",
        "![](https://t1.daumcdn.net/cfile/tistory/99954B335F4E638E2D)\n",
        "\n",
        "In addition to the produced output, partial convolutions propagate the updated binary mask to the following layers.  \n",
        "If the convolution was able to condition its output on at least one valid input, then the mask is removed at that location.  \n",
        "The result of this is that with a sufficiently deep network, the mask will eventually be all ones (i.e. disappear).\n",
        "![](https://drive.google.com/uc?export=view&id=1Gody9BVYBNUsXsLttYk37f8cB82FA_-0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "321bab8e-6424-41ff-8584-83124011e4ea",
      "metadata": {
        "id": "321bab8e-6424-41ff-8584-83124011e4ea"
      },
      "source": [
        "We will use [Nvidia's official implementation](https://github.com/NVIDIA/partialconv/blob/master/models/partialconv2d.py) of Partial convolution.  \n",
        "Do not necessarily pay attention to implementation details. Just remember that these layers have an additional element in the forward function: they take as input an additional mask and produce as output the result of the partial convolution and the updated mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cbe5efb-5f94-4ae4-ba8a-cd65fcfee4fe",
      "metadata": {
        "id": "2cbe5efb-5f94-4ae4-ba8a-cd65fcfee4fe"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# BSD 3-Clause License\n",
        "#\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.\n",
        "#\n",
        "# Author & Contact: Guilin Liu (guilinl@nvidia.com)\n",
        "###############################################################################\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, cuda\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class PartialConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # whether the mask is multi-channel or not\n",
        "        if 'multi_channel' in kwargs:\n",
        "            self.multi_channel = kwargs['multi_channel']\n",
        "            kwargs.pop('multi_channel')\n",
        "        else:\n",
        "            self.multi_channel = False\n",
        "\n",
        "        if 'return_mask' in kwargs:\n",
        "            self.return_mask = kwargs['return_mask']\n",
        "            kwargs.pop('return_mask')\n",
        "        else:\n",
        "            self.return_mask = False\n",
        "\n",
        "        super(PartialConv2d, self).__init__(*args, **kwargs)\n",
        "\n",
        "        if self.multi_channel:\n",
        "            self.weight_maskUpdater = torch.ones(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
        "        else:\n",
        "            self.weight_maskUpdater = torch.ones(1, 1, self.kernel_size[0], self.kernel_size[1])\n",
        "\n",
        "        self.slide_winsize = self.weight_maskUpdater.shape[1] * self.weight_maskUpdater.shape[2] * self.weight_maskUpdater.shape[3]\n",
        "\n",
        "        self.last_size = (None, None, None, None)\n",
        "        self.update_mask = None\n",
        "        self.mask_ratio = None\n",
        "\n",
        "    def forward(self, input, mask_in=None):\n",
        "        assert len(input.shape) == 4\n",
        "        if mask_in is not None or self.last_size != tuple(input.shape):\n",
        "            self.last_size = tuple(input.shape)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if self.weight_maskUpdater.type() != input.type():\n",
        "                    self.weight_maskUpdater = self.weight_maskUpdater.to(input)\n",
        "\n",
        "                if mask_in is None:\n",
        "                    # if mask is not provided, create a mask\n",
        "                    if self.multi_channel:\n",
        "                        mask = torch.ones(input.data.shape[0], input.data.shape[1], input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                    else:\n",
        "                        mask = torch.ones(1, 1, input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                else:\n",
        "                    mask = mask_in\n",
        "\n",
        "                self.update_mask = F.conv2d(mask, self.weight_maskUpdater, bias=None, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=1)\n",
        "\n",
        "                # for mixed precision training, change 1e-8 to 1e-6\n",
        "                self.mask_ratio = self.slide_winsize/(self.update_mask + 1e-8)\n",
        "                # self.mask_ratio = torch.max(self.update_mask)/(self.update_mask + 1e-8)\n",
        "                self.update_mask = torch.clamp(self.update_mask, 0, 1)\n",
        "                self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)\n",
        "\n",
        "\n",
        "        raw_out = super(PartialConv2d, self).forward(torch.mul(input, mask) if mask_in is not None else input)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            bias_view = self.bias.view(1, self.out_channels, 1, 1)\n",
        "            output = torch.mul(raw_out - bias_view, self.mask_ratio) + bias_view\n",
        "            output = torch.mul(output, self.update_mask)\n",
        "        else:\n",
        "            output = torch.mul(raw_out, self.mask_ratio)\n",
        "\n",
        "\n",
        "        if self.return_mask:\n",
        "            return output, self.update_mask\n",
        "        else:\n",
        "            return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a877d67-1dff-45c0-b7b9-2cb3279f5b28",
      "metadata": {
        "id": "1a877d67-1dff-45c0-b7b9-2cb3279f5b28"
      },
      "source": [
        "The authors use in their paper a U-Net whose convolutions are replaced by partial convolutions.  \n",
        "The masks are thus sent to the following layers but also to the decoder layers through skip connections.\n",
        "\n",
        "![](http://joshpatel.ca/images/Image-Inpainting/unet.jpg)\n",
        "Source(http://joshpatel.ca/image_inpainting)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99e73b01-d96b-4e7a-9442-68a6c562466b",
      "metadata": {
        "id": "99e73b01-d96b-4e7a-9442-68a6c562466b"
      },
      "source": [
        "Modify our previous U-Net architecture to use partial convolutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99eb1041-66e8-489d-8df5-76b544f42185",
      "metadata": {
        "id": "99eb1041-66e8-489d-8df5-76b544f42185"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv1 = PartialConv2d(..., return_mask=True)\n",
        "        self.conv2 = PartialConv2d(..., return_mask=True)\n",
        "        self.activtion = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, x, m):\n",
        "        x, m = ...\n",
        "        x = self.activtion(x)\n",
        "        x, m = ...\n",
        "        x = self.activtion(x)\n",
        "        return x, m\n",
        "\n",
        "\n",
        "\n",
        "class DownSampleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_block = ...\n",
        "        self.maxpool = ...\n",
        "\n",
        "    def forward(self, x, m):\n",
        "        x_skip, m_skip = ...\n",
        "        x = ...\n",
        "        m = ...\n",
        "\n",
        "        return x, m , x_skip, m_skip\n",
        "\n",
        "class UpSampleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_block = ...\n",
        "        self.upsample = ...\n",
        "\n",
        "    def forward(self, x, m, x_skip, m_skip):\n",
        "        x = ...\n",
        "        m = ...\n",
        "        x = ...\n",
        "        x, m = ...\n",
        "\n",
        "        return x, m\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample_block_1 = ...\n",
        "        self.downsample_block_2 = ...\n",
        "        self.middle_conv_block = ...\n",
        "\n",
        "\n",
        "        self.upsample_block_2 = ...\n",
        "        self.upsample_block_1 = ...\n",
        "\n",
        "        self.last_conv = ...\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Generate mask\n",
        "        m = ((x[:,:1] != 0)*1.)\n",
        "\n",
        "        x, m, x_skip1, m_skip1 = ...\n",
        "        x, m, x_skip2, m_skip2 = ...\n",
        "\n",
        "        x, m = ...\n",
        "\n",
        "        x, m = ...\n",
        "        x, m = ...\n",
        "\n",
        "        out = ...\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv1 = PartialConv2d(in_channels, out_channels, 3, padding=1, return_mask=True)\n",
        "        self.conv2 = PartialConv2d(out_channels, out_channels, 3, padding=1, return_mask=True)\n",
        "        self.activtion = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, m):\n",
        "        x, m = self.conv1(x, m)\n",
        "        x = self.activtion(x)\n",
        "        x, m = self.conv2(x, m)\n",
        "        x = self.activtion(x)\n",
        "        return x, m\n",
        "\n",
        "\n",
        "\n",
        "class DownSampleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_block = DoubleConv(in_channels, out_channels)\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x, m):\n",
        "        x_skip, m_skip = self.conv_block(x, m)\n",
        "        x = self.maxpool(x_skip)\n",
        "        m = self.maxpool(m)\n",
        "\n",
        "        return x, m , x_skip, m_skip\n",
        "\n",
        "class UpSampleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_block = DoubleConv(in_channels, out_channels)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "    def forward(self, x, m, x_skip, m_skip):\n",
        "        x = self.upsample(x)\n",
        "        m = self.upsample(m)\n",
        "        x = torch.cat([x, x_skip], dim=1)\n",
        "#        m = torch.cat([m, m_skip], dim=1)\n",
        "        x, m = self.conv_block(x, m)\n",
        "\n",
        "        return x, m\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample_block_1 = DownSampleBlock(3, 64)\n",
        "        self.downsample_block_2 = DownSampleBlock(64, 128)\n",
        "        self.middle_conv_block = DoubleConv(128, 256)\n",
        "\n",
        "\n",
        "        self.upsample_block_2 = UpSampleBlock(128 + 256, 128)\n",
        "        self.upsample_block_1 = UpSampleBlock(128 + 64, 64)\n",
        "\n",
        "        self.last_conv = nn.Conv2d(64, 3, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Generate mask\n",
        "        m = ((x[:,:1] != 0)*1.)\n",
        "\n",
        "        x, m, x_skip1, m_skip1 = self.downsample_block_1(x, m)\n",
        "        x, m, x_skip2, m_skip2 = self.downsample_block_2(x, m)\n",
        "\n",
        "        x, m = self.middle_conv_block(x, m)\n",
        "\n",
        "        x, m = self.upsample_block_2(x, m, x_skip2, m_skip2)\n",
        "        x, m = self.upsample_block_1(x, m, x_skip1, m_skip1)\n",
        "\n",
        "        out = self.last_conv(x)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2-8SBzfWfWYJ"
      },
      "id": "2-8SBzfWfWYJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cb149cd6-d483-49b5-a22b-627bba189d4c",
      "metadata": {
        "id": "cb149cd6-d483-49b5-a22b-627bba189d4c"
      },
      "source": [
        "Verify that the defined architecture, outputs a tensor with the same dimensions as its inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "700ed4b5-bf80-41fc-b898-4dc0518ee73f",
      "metadata": {
        "id": "700ed4b5-bf80-41fc-b898-4dc0518ee73f"
      },
      "outputs": [],
      "source": [
        "model = UNet()\n",
        "y = model(x)\n",
        "print(f'Input shape: {x.shape}')\n",
        "print(f'Reconstruction shape: {y.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd8c754d-d05e-4334-9d2e-44e366a9e803",
      "metadata": {
        "id": "cd8c754d-d05e-4334-9d2e-44e366a9e803"
      },
      "source": [
        "In the article the authors separate the loss of reconstruction into two parts, one on the missing parts of the image and the other on its complement.  \n",
        "This allows them to weight the importance given to each of these terms.  \n",
        "For the sake of simplicity, we will continue to use a single global loss for training.  \n",
        "Use your implementtaion of U-Net with partial convolutions to train a new network to fill the missing parts of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf85cadf-57f5-45a7-9eab-15194c7bca14",
      "metadata": {
        "id": "bf85cadf-57f5-45a7-9eab-15194c7bca14"
      },
      "outputs": [],
      "source": [
        "model = UNet().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "train_inpainting(model, optimizer, trainloader, epochs=5, min_cut=4, max_cut=60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Trained model results\n",
        "model = UNet().to(device)\n",
        "model.load_state_dict(torch.load('models/partial_conv.pth'))\n",
        "x, _ = next(iter (testloader))\n",
        "x_prime = random_cutout(x, 4, 60)\n",
        "x_hat = model(x_prime.cuda())\n",
        "plot_img(x[:8])\n",
        "plot_img(x_prime[:8])\n",
        "plot_img(torch.clip(x_hat[:8], 0, 1))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-534NM2wYXnU"
      },
      "id": "-534NM2wYXnU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e37aea4e-8f0d-42ad-b716-c3e8c3c17091",
      "metadata": {
        "id": "e37aea4e-8f0d-42ad-b716-c3e8c3c17091"
      },
      "source": [
        "# Losses\n",
        "## L1 loss\n",
        "So far, we have used a Mean Squared Error Loss to compute the pixel differences between our prediction and the desired images.  \n",
        "Minimizing the mean per-pixel squared difference may result in blurry images.  \n",
        "The problem is in the shape of the loss as it approaches zero. The closer the error is to zero, the smaller the gradient is, meaning that a slight deviation from the ground truth necessary for sharpness is not penalized as much.\n",
        "An alternative consists in using an L1 distance instead, which has constant gradients that could lead to sharper-looking images.\n",
        "\n",
        "## Perceptual losses\n",
        "Recently, a new category off loss functions from the style transfer community has been combined with other super-resolution techniques, leading to significantly improved output image quality.  \n",
        "In particular, the __Perceptual Losses__, introduced in [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155) consists in VGG network pre-trained for image classification to minimize two terms:\n",
        "* a __content loss__ consisting in the L2 difference between the features of the produced image and the reference image when passed trough the pre-trained network. The intuition behind this loss is to preserve a similar high-level feature representation leading to better semantic correctness of the completion.\n",
        "\n",
        "* a __style loss__ consisting in the L2 difference the auto-correlations of the feature maps also computed with the pre-trained network.  The auto-correlations are represented by [Gram Matrices](https://en.wikipedia.org/wiki/Gram_matrix) and are supposed to contain the style information of an image such as textures and colours.\n",
        "\n",
        "![](https://images.deepai.org/django-summernote/2019-06-16/54b7a304-efa9-46fb-8034-c750dcd98c99.png)  \n",
        "\n",
        "The following code implements a network that outputs 4 groups of features of a pre-trained VGG16 and a function computing the perceptual losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c2e632-729c-4809-9faf-54fd1c7c9539",
      "metadata": {
        "id": "b7c2e632-729c-4809-9faf-54fd1c7c9539"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "def gram_matrix(input):\n",
        "    a, b, c, d = input.size()   # a=batch size(=1)\n",
        "                                # b=number of feature maps\n",
        "                                # (c,d)=dimensions of a f. map (N=c*d)\n",
        "\n",
        "    features = input.view(a * b, c * d)\n",
        "    G = torch.mm(features, features.t())  # compute the gram product\n",
        "    # we 'normalize' the values of the gram matrix\n",
        "    # by dividing by the number of element in each feature maps.\n",
        "    return G.div(a * b * c * d)\n",
        "\n",
        "class LossNetwork(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LossNetwork, self).__init__()\n",
        "        vgg_model = models.vgg16(pretrained=True).to(device).eval()\n",
        "        self.vgg_layers = vgg_model.features\n",
        "        self.layer_name_mapping = {\n",
        "            '3': \"relu1_2\",\n",
        "            '8': \"relu2_2\",\n",
        "            '15': \"relu3_3\",\n",
        "            '22': \"relu4_3\"\n",
        "        }\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for name, module in self.vgg_layers._modules.items():\n",
        "            x = module(x)\n",
        "            if name in self.layer_name_mapping:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "loss_network = LossNetwork()\n",
        "\n",
        "def perceptual_loss(x, y):\n",
        "    mse = torch.nn.MSELoss()\n",
        "    x_feats = loss_network(x)\n",
        "    y_feats = loss_network(y)\n",
        "    #content loss:\n",
        "    loss = mse(x_feats[2], y_feats[2].detach())\n",
        "    #style loss:\n",
        "    for feats, target_feats in zip(x_feats, y_feats):\n",
        "        loss += mse(gram_matrix(feats), gram_matrix(target_feats.detach()))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a30a57d-f02d-4055-8d3d-0e348cf215cd",
      "metadata": {
        "id": "5a30a57d-f02d-4055-8d3d-0e348cf215cd"
      },
      "source": [
        "## Total variation loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc083b61-7cdc-4fd0-8c43-c61638fccebe",
      "metadata": {
        "id": "bc083b61-7cdc-4fd0-8c43-c61638fccebe"
      },
      "source": [
        "To encourage the smoothness of the completed images, a regularization term called __total variation loss__ is often added to the final objective function.\n",
        "This loss is commonly used in the signal processing community as a noise removal process (filter).\n",
        "It simply consists in computing the sum of the absolute differences for neighboring pixel-values in the input images.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db02bb1-ed4f-4b36-852f-f875c2988f80",
      "metadata": {
        "id": "8db02bb1-ed4f-4b36-852f-f875c2988f80"
      },
      "outputs": [],
      "source": [
        "def total_variation_loss(x):\n",
        "    loss = torch.mean(torch.abs(x[:, :, :, :-1] - x[:, :, :, 1:])) + \\\n",
        "            torch.mean(torch.abs(x[:, :, :-1, :] - x[:, :, 1:, :]))\n",
        "    return torch.mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e2a714-4f3b-468b-a995-bf6b611b2355",
      "metadata": {
        "id": "77e2a714-4f3b-468b-a995-bf6b611b2355"
      },
      "source": [
        "Modify the ```train_upscale``` and the ```train_inpainting``` methods to use a L1 loss instead of the MSE and to incorporate the perceptual and total variation losses.  \n",
        "Train a Subpixel Network to improve the images quality and a Unet with partial convolution to fill the missed parts of images using these enhanced training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b85eb3d3-ef4b-4e25-8224-f8913a3a6492",
      "metadata": {
        "id": "b85eb3d3-ef4b-4e25-8224-f8913a3a6492"
      },
      "outputs": [],
      "source": [
        "def train_upscale(net, optimizer, loader, epochs=5, scale_factor=2, upscale=True):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "def train_upscale(net, optimizer, loader, epochs=5, scale_factor=2, upscale=True):\n",
        "    criterion = torch.nn.L1Loss()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = []\n",
        "        t = tqdm(loader)\n",
        "        for x, _ in t:\n",
        "            x = x.to(device)\n",
        "            x_low_res = downscale(x, scale_factor, upscale)\n",
        "            outputs = net(x_low_res)\n",
        "            loss = criterion(outputs, x) + perceptual_loss(outputs, x) + total_variation_loss(outputs)\n",
        "            running_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f'training loss: {mean(running_loss)}')\n",
        "        plot_img(x[:8])\n",
        "        plot_img(x_low_res[:8])\n",
        "        plot_img(torch.clip(outputs[:8], 0, 1))"
      ],
      "metadata": {
        "id": "JmoeMM7Sgomp"
      },
      "id": "JmoeMM7Sgomp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56a0d632-6689-423e-8f92-f5c7e015d71e",
      "metadata": {
        "id": "56a0d632-6689-423e-8f92-f5c7e015d71e"
      },
      "outputs": [],
      "source": [
        "def train_inpainting(net, optimizer, loader, epochs=5, min_cut=15,max_cut=45):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "def train_inpainting(net, optimizer, loader, epochs=5, min_cut=15,max_cut=45):\n",
        "    criterion = torch.nn.L1Loss()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = []\n",
        "        t = tqdm(loader)\n",
        "        for x, _ in t:\n",
        "            x = x.to(device)\n",
        "            x_co = random_cutout(x, min_cut, max_cut)\n",
        "            outputs = net(x_co)\n",
        "            loss = criterion(outputs, x) + perceptual_loss(outputs, x) + total_variation_loss(outputs)\n",
        "            running_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f'training loss: {mean(running_loss)}')\n",
        "        plot_img(x[:8])\n",
        "        plot_img(x_co[:8])\n",
        "        plot_img(torch.clip(outputs[:8], 0, 1))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uFL8jvJJgyjm"
      },
      "id": "uFL8jvJJgyjm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b531bdb-f7f0-4fb1-8173-9dc5128637c7",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "3b531bdb-f7f0-4fb1-8173-9dc5128637c7"
      },
      "outputs": [],
      "source": [
        "#Since we use a separate pre-trained network to compute our perceptual losses, the whole training loop uses more GPU memory.\n",
        "#We need to reduce the batch size to fit in the GPU memory.\n",
        "trainloader, testloader = get_faces_loader(path='data/lfw',\n",
        "                                    batch_size=32,\n",
        "                                    shuffle=True,\n",
        "                                    num_workers=2)\n",
        "\n",
        "model = SubPixelNetwork(2).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train_upscale(model, optimizer, trainloader, epochs=10, upscale=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Trained model results\n",
        "model = SubPixelNetwork(2).cuda()\n",
        "model.load_state_dict(torch.load('models/super_res_all_losses.pth'))\n",
        "x, _ = next(iter (testloader))\n",
        "x_prime = downscale(x, 2, False)\n",
        "x_hat = model(x_prime.cuda())\n",
        "plot_img(x[:8])\n",
        "plot_img(x_prime[:8])\n",
        "plot_img(torch.clip(x_hat[:8], 0, 1))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KyFxd0CnY4PF"
      },
      "id": "KyFxd0CnY4PF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7399d5f-8bc4-425d-b9d6-3eaefe401e14",
      "metadata": {
        "id": "b7399d5f-8bc4-425d-b9d6-3eaefe401e14"
      },
      "outputs": [],
      "source": [
        "#Since we use a separate pre-trained network to compute our perceptual losses, the whole training loop uses more GPU memory.\n",
        "#We need to reduce the batch size to fit in the GPU memory.\n",
        "trainloader, testloader = get_faces_loader(path='data/lfw',\n",
        "                                    batch_size=32,\n",
        "                                    shuffle=True,\n",
        "                                    num_workers=2)\n",
        "model = UNet().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "train_inpainting(model, optimizer, trainloader, epochs=10, min_cut=4, max_cut=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1309fd5b-1b15-472d-b602-ab435deb8136",
      "metadata": {
        "id": "1309fd5b-1b15-472d-b602-ab435deb8136",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Trained model results\n",
        "model = UNet().to(device)\n",
        "model.load_state_dict(torch.load('models/inpainting_all_losses.pth'))\n",
        "x, _ = next(iter (testloader))\n",
        "x_prime = random_cutout(x, 4, 60)\n",
        "x_hat = model(x_prime.cuda())\n",
        "plot_img(x[:8])\n",
        "plot_img(x_prime[:8])\n",
        "plot_img(torch.clip(x_hat[:8], 0, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d4515e2-3e55-4278-9049-b2470bba2b38",
      "metadata": {
        "id": "8d4515e2-3e55-4278-9049-b2470bba2b38"
      },
      "source": [
        "# Combining inpainting and super-resolution\n",
        "\n",
        "Finally, it is also possible to improve the prediction of an inpating network by using a super-resolution network.  \n",
        "In general the inpating network is trained separately and fine tuned during the training of the super-resolution network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afef29c8-fb82-4f2a-9b7d-75858d6ffcbe",
      "metadata": {
        "id": "afef29c8-fb82-4f2a-9b7d-75858d6ffcbe"
      },
      "outputs": [],
      "source": [
        "def train_impate_upscale(impainter, augmeter, optimizer, loader, epochs=5, min_cut=15,max_cut=45):\n",
        "    criterion = torch.nn.L1Loss()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = []\n",
        "        t = tqdm(loader)\n",
        "        for x, _ in t:\n",
        "            x = x.to(device)\n",
        "            x_co = random_cutout(x, min_cut, max_cut)\n",
        "            outputs = impainter(x_co)\n",
        "            outputs = augmeter(outputs)\n",
        "            loss = criterion(outputs, x) + perceptual_loss(outputs, x) + total_variation_loss(outputs)\n",
        "            running_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f'training loss: {mean(running_loss)}')\n",
        "        plot_img(x[:8])\n",
        "        plot_img(x_co[:8])\n",
        "        plot_img(torch.clip(outputs[:8], 0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e282b5fe-16ee-44b5-9cf0-50cb5a1ef74b",
      "metadata": {
        "tags": [],
        "id": "e282b5fe-16ee-44b5-9cf0-50cb5a1ef74b"
      },
      "outputs": [],
      "source": [
        "augmeter = SubPixelNetwork(1).cuda()\n",
        "optimizer = torch.optim.Adam(augmeter.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train_upscale(augmeter, optimizer, trainloader, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b59e0621-a93f-4038-91ec-311690f7cd9b",
      "metadata": {
        "id": "b59e0621-a93f-4038-91ec-311690f7cd9b"
      },
      "outputs": [],
      "source": [
        "inpainter = model\n",
        "#params = list(inpainter.parameters()) + list(augmeter.parameters())\n",
        "\n",
        "#optimizer = torch.optim.Adam(params, lr=1e-5, weight_decay=0.001)\n",
        "#augmeter = SubPixelNetwork(1).cuda()\n",
        "optimizer = torch.optim.Adam(augmeter.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "train_impate_upscale(inpainter, augmeter, optimizer, trainloader, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Trained model results\n",
        "augmenter = SubPixelNetwork(1).cuda()\n",
        "augmenter.load_state_dict(torch.load('models/augmenter.pth'))\n",
        "inpainter = UNet().cuda()\n",
        "inpainter.load_state_dict(torch.load('models/inpainter.pth'))\n",
        "x, _ = next(iter (testloader))\n",
        "x_prime = random_cutout(x, 4, 60)\n",
        "x_hat = augmenter(inpainter(x_prime.cuda()))\n",
        "plot_img(x[:8])\n",
        "plot_img(x_prime[:8])\n",
        "plot_img(torch.clip(x_hat[:8], 0, 1))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QULSr-1sZCKG"
      },
      "id": "QULSr-1sZCKG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aa6a755e-c51e-4cd8-ac32-52e2c01beaeb",
      "metadata": {
        "id": "aa6a755e-c51e-4cd8-ac32-52e2c01beaeb"
      },
      "source": [
        "# Optional: GAN\n",
        "We have seen that different loss functions allow to minimize the difference in perception between the generated image and the original image.  \n",
        "There is an infinite number of solutions to fill the missing parts of the images. The important thing is that the filled part seems visually coherent.  \n",
        "To do this, one can use a separate neural network that has been trained to recognize real images from fake ones.\n",
        "This approach, better known as Generative Adverserial Networks (GAN) has been used in inpainting in [Context Encoders: Feature Learning by Inpainting](https://arxiv.org/abs/1604.07379) to improve the prediction of an auto-encoder.  \n",
        "The auto-encoder then plays the role of generator and is trained to fill in the missing parts using a reconstruction loss but also a signal provided by a discriminator able to train to discern the true images from the false ones.\n",
        "In this way, the missing parts of the image are constructed by the generator to produce plausible pieces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e540dbc-c01f-4d24-a145-064b83522b31",
      "metadata": {
        "id": "1e540dbc-c01f-4d24-a145-064b83522b31"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets.folder import ImageFolder\n",
        "\n",
        "def get_super_resolution_loader(path, **kwargs):\n",
        "    process = transforms.Compose(\n",
        "      [transforms.Resize((120, 120)), transforms.CenterCrop((64, 64)),transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    dataset = ImageFolder(path, process)\n",
        "    lengths = [12000, 1233]\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, lengths)\n",
        "    return DataLoader(train_set, **kwargs), DataLoader(val_set, **kwargs)\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "              [transforms.Resize((120, 120)),\n",
        "               transforms.CenterCrop((64, 64)),\n",
        "               transforms.ToTensor(),\n",
        "               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "dataset = ImageFolder('data/lfw', transform)\n",
        "trainloader =  DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean= [-m/s for m, s in zip((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))],\n",
        "   std= [1/s for s in (0.5, 0.5, 0.5)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e61bbe5-414a-4cd0-ae00-d66f65c23162",
      "metadata": {
        "id": "4e61bbe5-414a-4cd0-ae00-d66f65c23162"
      },
      "outputs": [],
      "source": [
        "generator = nn.Sequential(UNet(), nn.Tanh()).to(device)\n",
        "\n",
        "discriminator = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        ).to(device)\n",
        "\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "discriminator.apply(weights_init);\n",
        "generator.apply(weights_init);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97349898-fc7c-4ed8-b18c-4a336db01909",
      "metadata": {
        "id": "97349898-fc7c-4ed8-b18c-4a336db01909"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "\n",
        "real_label = 0.9\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "lr = 0.001\n",
        "beta1 = 0.5\n",
        "optimizerD = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizerG = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eebf31b4-070c-418a-8fa1-928d867f90ef",
      "metadata": {
        "id": "eebf31b4-070c-418a-8fa1-928d867f90ef"
      },
      "outputs": [],
      "source": [
        "def update_discriminator(discriminator, optimizer, x, fake, real_label=1, fake_label=0):\n",
        "    discriminator.zero_grad()\n",
        "    b_size = x.size(0)\n",
        "    label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "    # Forward pass real batch through D\n",
        "    output = discriminator(x).view(-1)\n",
        "    # Calculate loss on all-real batch\n",
        "    loss = criterion(output, label)\n",
        "    D_x = output.mean().item()\n",
        "    label = torch.full((b_size,), fake_label, dtype=torch.float, device=device)\n",
        "    # Classify all fake batch with D\n",
        "    output = discriminator(fake.detach()).view(-1)\n",
        "    # Calculate D's loss on the all-fake batch\n",
        "    loss += criterion(output, label)\n",
        "    D_G_z1 = output.mean().item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss, D_x, D_G_z1\n",
        "\n",
        "def update_generatot(generator, discriminator, optimizer, x, fake, real_label=1):\n",
        "    generator.zero_grad()\n",
        "    b_size = fake.size(0)\n",
        "    label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "    output = discriminator(fake).view(-1)\n",
        "    loss = nn.BCELoss()(output, label) + torch.nn.L1Loss()(fake, x)\n",
        "    loss.backward()\n",
        "    D_G_z2 = output.mean().item()\n",
        "    optimizer.step()\n",
        "    return loss, D_G_z2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd76226b-8d72-4b9a-b2ad-28756f74bee0",
      "metadata": {
        "id": "bd76226b-8d72-4b9a-b2ad-28756f74bee0"
      },
      "outputs": [],
      "source": [
        "def train_context_encoder(generator, discriminator, optimizerG, optimizerD, loader, epochs=10, real_label=0.9, fake_label=0):\n",
        "    for epoch in range(epochs):\n",
        "        t = tqdm(loader)\n",
        "        for x, _ in t:\n",
        "            x = x.to(device)\n",
        "            ## Train with all-fake batch\n",
        "            x_co = random_cutout(x, 5, 60)\n",
        "            fake = generator(x_co)\n",
        "\n",
        "        # discriminator update\n",
        "            errD, D_x, D_G_z1 = update_discriminator(discriminator, optimizerD, x, fake, real_label, fake_label)\n",
        "            errG, D_G_z2 = update_generatot(generator, discriminator, optimizerG, x, fake, real_label)\n",
        "\n",
        "\n",
        "        print(f'epoch:{epoch}/{epochs} \\tLoss_D:{errD.item():.4f} \\tLoss_G:{errG.item():.4f} \\tD(x):{D_x:.4f} \\tD(G(z)):{D_G_z1:.4f} / {D_G_z2:.4f}')\n",
        "        plot_img(inv_normalize(x[:8]))\n",
        "        plot_img(inv_normalize(x_co[:8]))\n",
        "        plot_img(torch.clip(inv_normalize(fake[:8]), 0, 1))\n",
        "\n",
        "train_context_encoder(generator, discriminator, optimizerG, optimizerD, trainloader, real_label=0.9, fake_label=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c2a2280-d357-414c-b686-f60e09088fa3",
      "metadata": {
        "id": "8c2a2280-d357-414c-b686-f60e09088fa3"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    x, _ = next(iter(trainloader))\n",
        "    x_co = random_cutout(x, 5, 60).to(device)\n",
        "    fake = generator(x_co).detach().cpu()\n",
        "plot_img(inv_normalize(x[:8]))\n",
        "plot_img(inv_normalize(x_co[:8]))\n",
        "plot_img(torch.clip(inv_normalize(fake[:8]), 0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator\n"
      ],
      "metadata": {
        "id": "Girz3KuOa4bJ"
      },
      "id": "Girz3KuOa4bJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "85c3bd64-4f1c-4841-9f2d-12849af39b38",
      "metadata": {
        "id": "85c3bd64-4f1c-4841-9f2d-12849af39b38"
      },
      "source": [
        "# Optional: additional methods.\n",
        "## Edges prior\n",
        "\n",
        "To further improve the quality of the inpainting results, we can provide prior information of the missing regions to the generator network to generate better local fine texture details.  \n",
        "This is done in the article [EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning](https://arxiv.org/pdf/1901.00212.pdf) where a generator is trained to produce the cany edges with missed regions filled.  \n",
        "This edges representation is then combined with the original image and used by a second generator to produce the final output.  \n",
        "![](https://miro.medium.com/max/2400/1*KKiBBWo20W2BjrzEViWVVA.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a479ae9-b12a-49ac-8382-54cb637fd426",
      "metadata": {
        "id": "8a479ae9-b12a-49ac-8382-54cb637fd426"
      },
      "outputs": [],
      "source": [
        "# This code will download a cany-edges version of the dataset pre-computed  using Open CV.\n",
        "\n",
        "!wget https://github.com/DavidBert/INSA_notebooks/raw/main/lfw_cany.zip\n",
        "!unzip lfw_cany.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ed8a20-11d1-46ee-b7ac-93eba2025629",
      "metadata": {
        "id": "b5ed8a20-11d1-46ee-b7ac-93eba2025629"
      },
      "outputs": [],
      "source": [
        "  from torchvision.datasets.folder import ImageFolder, default_loader, IMG_EXTENSIONS\n",
        "\n",
        "class ImagesAndEdges(ImageFolder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        transform=None,\n",
        "        target_transform=None,\n",
        "    ):\n",
        "        super(ImageFolder, self).__init__(\n",
        "            root=root,\n",
        "            loader=default_loader,\n",
        "            transform=transform,\n",
        "            extensions=IMG_EXTENSIONS,\n",
        "        )\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, _ = self.samples[index]\n",
        "        cany_path = path.replace(\"lfw\", \"lfw_cany\")\n",
        "        sample = self.loader(path)\n",
        "        cany_sample = self.loader(cany_path)\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "            cany_sample = self.transform(cany_sample)\n",
        "        #cany_sample = cany_sample.mean(0).unsqueeze(0)\n",
        "        return sample, cany_sample\n",
        "\n",
        "def get_images_and_edges_loader(path, **kwargs):\n",
        "  process = transforms.Compose(\n",
        "      [transforms.Resize((120, 120)), transforms.CenterCrop((64, 64)),transforms.ToTensor()])\n",
        "  dataset = ImagesAndEdges(path, process)\n",
        "  lengths = [12000, 1233]\n",
        "  train_set, val_set = torch.utils.data.random_split(dataset, lengths)\n",
        "  return DataLoader(train_set, **kwargs), DataLoader(val_set, **kwargs)\n",
        "\n",
        "trainloader, testloader = get_images_and_edges_loader(path='data/lfw',\n",
        "                                    batch_size=128,\n",
        "                                    shuffle=True,\n",
        "                                    num_workers=2)\n",
        "\n",
        "\n",
        "x, y = next(iter(trainloader))\n",
        "plot_img(x[:8])\n",
        "plot_img(y[:8])\n",
        "plot_img(add_noise(x[:8]))\n",
        "plot_img(add_noise(y[:8]))\n",
        "plot_img(downscale(x[:8]))\n",
        "plot_img(downscale(y[:8]))\n",
        "x_co = random_cutout(x, 4, 60)\n",
        "plot_img(x_co[:8])\n",
        "y_co = y * (x_co[:,:1] != 0)*1.\n",
        "plot_img(y_co[:8])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee544032-503c-4bf3-9bf3-dee94f64ffdb",
      "metadata": {
        "id": "ee544032-503c-4bf3-9bf3-dee94f64ffdb"
      },
      "source": [
        "The paper uses GANs to train both generators.  \n",
        "Let's first train the edge network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ca7660-aa5e-4ec1-93b6-cd72dc121539",
      "metadata": {
        "tags": [],
        "id": "e2ca7660-aa5e-4ec1-93b6-cd72dc121539"
      },
      "outputs": [],
      "source": [
        "edge_generator = nn.Sequential(UNet(in_channels=3, out_channels=1).cuda(), nn.Tanh()).to(device)\n",
        "\n",
        "discriminator = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        ).to(device)\n",
        "\n",
        "\n",
        "discriminator.apply(weights_init);\n",
        "edge_generator.apply(weights_init);"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "\n",
        "real_label = 0.9\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "lr = 0.001\n",
        "beta1 = 0.5\n",
        "optimizerD = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizerG = torch.optim.Adam(edge_generator.parameters(), lr=lr, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "Jc2hCvLoJeIp"
      },
      "id": "Jc2hCvLoJeIp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_context_encoder_edges(generator, discriminator, optimizerG, optimizerD, loader, epochs=10, real_label=0.9, fake_label=0, show_images=True):\n",
        "    for epoch in range(epochs):\n",
        "        t = tqdm(loader)\n",
        "        for x, y in t:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            ## Train with all-fake batch\n",
        "            x_co = random_cutout(x, 5, 60)\n",
        "            fake = generator(x_co)\n",
        "\n",
        "                    # discriminator update\n",
        "            errD, D_x, D_G_z1 = update_discriminator(discriminator, optimizerD, y, fake, real_label, fake_label)\n",
        "            errG, D_G_z2 = update_generatot(generator, discriminator, optimizerG, y, fake, real_label)\n",
        "\n",
        "\n",
        "        print(f'epoch:{epoch}/{epochs} \\tLoss_D:{errD.item():.4f} \\tLoss_G:{errG.item():.4f} \\tD(x):{D_x:.4f} \\tD(G(z)):{D_G_z1:.4f} / {D_G_z2:.4f}')\n",
        "        if show_images:\n",
        "            plot_img(x[:8])\n",
        "            plot_img(x_co[:8])\n",
        "            plot_img(y[:8])\n",
        "            plot_img(torch.clip(fake[:8], 0, 1))\n",
        "\n",
        "train_context_encoder_edges(edge_generator, discriminator, optimizerG, optimizerD, trainloader, epochs=3, real_label=0.9, fake_label=0)\n",
        "\n",
        "#train_context_encoder_edges(edge_generator, discriminator, optimizerG, optimizerD, trainloader, epochs=50, real_label=0.9, fake_label=0, show_images=False)\n",
        "#torch.save(edge_generator.state_dict(), 'models/edge_generator.pth')"
      ],
      "metadata": {
        "id": "ZU5ZwzVaJfsh"
      },
      "id": "ZU5ZwzVaJfsh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "22509b6e-0a35-4c5d-8b25-464a96266c07",
      "metadata": {
        "id": "22509b6e-0a35-4c5d-8b25-464a96266c07"
      },
      "source": [
        "We can now train the final model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2a76de-44d6-4958-8bfc-7f2e3f21f17a",
      "metadata": {
        "tags": [],
        "id": "6c2a76de-44d6-4958-8bfc-7f2e3f21f17a"
      },
      "outputs": [],
      "source": [
        "generator = nn.Sequential(UNet(in_channels=4, out_channels=3).cuda(), nn.Tanh()).to(device)\n",
        "\n",
        "discriminator = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        ).to(device)\n",
        "\n",
        "\n",
        "discriminator.apply(weights_init);\n",
        "generator.apply(weights_init);\n",
        "optimizerD = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizerG = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "class EdgeConnectModel(nn.Module):\n",
        "\n",
        "    def __init__(self, edge_generator, generator):\n",
        "        super().__init__()\n",
        "        self.edge_generator = edge_generator\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            edges = self.edge_generator(x)\n",
        "        x = torch.cat([x, edges.detach()], dim=1)\n",
        "        y = self.generator(x)\n",
        "        return y, edges\n",
        "\n",
        "model = EdgeConnectModel(edge_generator, generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abb3fc8-4a27-4528-9f71-c6f7c138accd",
      "metadata": {
        "id": "5abb3fc8-4a27-4528-9f71-c6f7c138accd"
      },
      "outputs": [],
      "source": [
        "def train_edge_connect_model(generator, discriminator, optimizerG, optimizerD, loader, epochs=10, real_label=0.9, fake_label=0, show_images=True):\n",
        "    for epoch in range(epochs):\n",
        "        t = tqdm(loader)\n",
        "        for x, _ in t:\n",
        "            x = x.to(device)\n",
        "            ## Train with all-fake batch\n",
        "            x_co = random_cutout(x, 5, 60)\n",
        "            fake, edges = generator(x_co)\n",
        "\n",
        "                    # discriminator update\n",
        "            errD, D_x, D_G_z1 = update_discriminator(discriminator, optimizerD, x, fake, real_label, fake_label)\n",
        "            errG, D_G_z2 = update_generatot(generator, discriminator, optimizerG, x, fake, real_label)\n",
        "\n",
        "\n",
        "        print(f'epoch:{epoch}/{epochs} \\tLoss_D:{errD.item():.4f} \\tLoss_G:{errG.item():.4f} \\tD(x):{D_x:.4f} \\tD(G(z)):{D_G_z1:.4f} / {D_G_z2:.4f}')\n",
        "        if show_images:\n",
        "            plot_img(x[:8])\n",
        "            plot_img(x_co[:8])\n",
        "            plot_img(edges[:8])\n",
        "            plot_img(torch.clip(fake[:8], 0, 1))\n",
        "\n",
        "train_edge_connect_model(model, discriminator, optimizerG, optimizerD, trainloader, epochs=3, real_label=0.9, fake_label=0)\n",
        "\n",
        "#train_edge_connect_model(model, discriminator, optimizerG, optimizerD, trainloader, epochs=50, real_label=0.9, fake_label=0, show_images=False)\n",
        "#torch.save(model.state_dict(), 'models/edge_connect_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('models/edge_connect_model.pth'))\n",
        "with torch.no_grad():\n",
        "    x, _ = next(iter (testloader))\n",
        "    x_prime = random_cutout(x, 4, 60)\n",
        "    x_hat, edges = model(x_prime.cuda())\n",
        "plot_img(x[:8])\n",
        "plot_img(x_prime[:8])\n",
        "plot_img(edges[:8])\n",
        "plot_img(x_hat[:8])"
      ],
      "metadata": {
        "id": "HGyHKkewba9d"
      },
      "id": "HGyHKkewba9d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "INSA_inpainting.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Vz_ufMNGXLpy"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}