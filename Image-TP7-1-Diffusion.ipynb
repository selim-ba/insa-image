{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3QUXwLwtDKG"
      },
      "source": [
        "# INSA, GMM Image\n",
        "## Practical sessions: Introduction to Diffusion\n",
        "\n",
        "Welcome in this practical session on diffusion models.  \n",
        "Diffusion models are a class of generative models that have been used to generate images, videos, and audio. They are based on the idea of gradually adding noise to an image and then removing the noise to get back the original image.  \n",
        "Since the training of diffusion models is computationally expensive, we will use a toy dataset to illustrate the concept of diffusion.  \n",
        "We will use a 2d points dataset to illustrate the concept of diffusion and then we will apply it to a pretrained image model.  \n",
        "Let's start by downloading and visualizing the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mK2eiTCWir0",
        "outputId": "bfa92a7b-3939-4b75-b5f2-f0a70aed9257"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/tanelp/tiny-diffusion/raw/refs/heads/master/static/DatasaurusDozen.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "AZNoqJeuW2Up",
        "outputId": "735bc89a-2d98-4431-dcfb-ed37d24b81bc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "df = pd.read_csv('DatasaurusDozen.tsv',sep='\\t')\n",
        "datasaurus = (np.asarray(df[df['dataset']=='dino'][['x','y']].values, dtype=float))\n",
        "datasaurus = (datasaurus - datasaurus.mean())/datasaurus.std()\n",
        "plt.scatter(datasaurus[:,0],datasaurus[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zovEk0LBtDKI"
      },
      "source": [
        "We will use this dataset to illustrate the concept of diffusion.  \n",
        "This data will be our initial distribution, the one we want to sample from.  \n",
        "Since we don't know how to sample from this distribution, we will use a noise schedule to gradually add noise to the data until we reach a Gaussian distribution for which we know how to sample.  \n",
        "Then we will use the reverse process to sample from the initial distribution.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvG33EWkXqIk"
      },
      "source": [
        "### Noise Schedule\n",
        "Let's begin by defining the noise schedule.  \n",
        "The noise schedule is a function that defines the amount of noise to add at each timestep.  \n",
        "We will use a linear schedule for this example.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "TqfYBhI0XWAM",
        "outputId": "29f58c56-357d-4377-e7de-01ad839a0cac"
      },
      "outputs": [],
      "source": [
        "T=200\n",
        "alpha_min=0.0001\n",
        "alpha_max=0.05\n",
        "alphas = torch.linspace(alpha_min, alpha_max, T)\n",
        "alphas = 1. - alphas\n",
        "alpha_bar = torch.cumprod(alphas, dim=0)\n",
        "plt.figure(figsize=[6, 6])\n",
        "plt.plot(torch.arange(T), alpha_bar)\n",
        "plt.xlabel('Timestep')\n",
        "plt.ylim(0,1.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKcuJH7WtDKI"
      },
      "source": [
        "### Forward Pass\n",
        "The forward pass is the process of adding noise to the data at each timestep.  \n",
        "We will use the noise schedule to define the amount of noise to add at each timestep.  \n",
        "We saw in class that the forward step can be written as:\n",
        "$$x_t = \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon_t$$\n",
        "where $\\alpha_t$ is the amount of noise to add at timestep $t$ and $\\epsilon_t$ is a noise sample.  \n",
        "Complete the following functions to perform the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYg1c48PoRSw"
      },
      "outputs": [],
      "source": [
        "def forward_step(x_t_minus_1:torch.Tensor, alphas:torch.Tensor, t:int, eps:torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Takes the previous step, the alphas and the timestep and returns the next step\n",
        "    args:\n",
        "        x_t_minus_1: the previous step\n",
        "        alphas: the alphas of the noise schedule\n",
        "        t: the timestep\n",
        "        eps: the noise sample\n",
        "    returns:\n",
        "        x_t: the next step\n",
        "    \"\"\"\n",
        "    x_t = alphas[t].sqrt()*x_t_minus_1 + (1-alphas[t]).sqrt()*eps\n",
        "    return x_t\n",
        "\n",
        "def forward_pass(x_0:torch.Tensor, alphas:torch.Tensor, T:int=200) -> List[torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Takes the initial data, the alphas and the number of timesteps and returns the list of forward steps\n",
        "    args:\n",
        "        x_0: the initial data\n",
        "        alphas: the alphas of the noise schedule\n",
        "        T: the number of timesteps\n",
        "    returns:\n",
        "        x_series: a list of the forward steps\n",
        "    \"\"\"\n",
        "    x_series = [x_0]\n",
        "    for t in range(T):\n",
        "        eps = torch.randn_like(x_0)\n",
        "        new_x = forward_step(x_series[-1],alphas,t,eps) # we use the alphas of the noise schedule\n",
        "        #new_x = forward_step(x_series[-1],torch.cumprod(alphas,dim=0),t,eps) # alpha_bar ?\n",
        "        x_series.append(new_x)\n",
        "    return x_series\n",
        "\n",
        "x_0 = torch.tensor(datasaurus).repeat(6, 1) # we repeat the data 6 times to have more points to visualize\n",
        "x_series = forward_pass(x_0, alphas, T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTa3E4EGtDKJ"
      },
      "source": [
        "Now plot the different steps of the forward pass for t = [0, 6, 12, 25, 50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "4nYrBUNttDKJ",
        "outputId": "b48aada8-b2d4-48dc-9cc7-c74555d24643"
      },
      "outputs": [],
      "source": [
        "figure = plt.figure(figsize=(20, 4))\n",
        "for i, t in enumerate([0, 6, 12, 25, 50]):\n",
        "    dataset, time_step = x_series[t], t\n",
        "    figure.add_subplot(1,5,i+1)\n",
        "    plt.title(time_step)\n",
        "    plt.axis(\"off\")\n",
        "    plt.scatter(dataset[:,0],dataset[:,1],s=15,alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuVJDPwjtDKJ"
      },
      "source": [
        "You should observe that the data becomes more and more noisy as the timestep increases.  \n",
        "The following code animates the forward pass and allows you to see the data evolve over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lalR9En6tDKJ",
        "outputId": "8b4b4a6d-dafa-4e8b-a7bc-a713820d1212"
      },
      "outputs": [],
      "source": [
        "from matplotlib.animation import FuncAnimation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from functools import partial\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "def animate(i:int, series:List[torch.Tensor]):\n",
        "    ax.clear()\n",
        "    data = series[i]\n",
        "    ax.scatter(data[:, 0], data[:, 1], s=15, alpha=0.5)\n",
        "    ax.set_axis_off()\n",
        "\n",
        "animate_forward = partial(animate, series=x_series)\n",
        "\n",
        "anim = FuncAnimation(fig, animate_forward, frames=len(x_series),\n",
        "                    interval=250)  # 500ms between frames\n",
        "\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P80puz4EtDKK"
      },
      "source": [
        "For training, we would like to have diversity in the training batches, meaning different samples with different timesteps.  \n",
        "We saw in class that it is to directly noise the data for a given timestep without having to go through the forward pass for all the timesteps.  \n",
        "$$x_t = \\sqrt{\\bar{\\alpha_t}}x_{0} + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_t$$\n",
        "Complete the following function to sample the data for a given timestep and verify that it seems correct by plotting the data for t in [0, 6, 12, 25, 50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "ood3rtnetDKK",
        "outputId": "ef521cd0-5e2f-4a7d-a484-5380844b3bf3"
      },
      "outputs": [],
      "source": [
        "def sample_x_t(x_0:torch.Tensor, t:int, alpha_bar:torch.Tensor, eps:torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Takes the initial data, the alphas and the number of timesteps and returns a noisy version of the data for a given timestep\n",
        "    args:\n",
        "        x_0: the initial data\n",
        "        alphas: the alphas of the noise schedule\n",
        "        T: the number of timesteps\n",
        "    returns:\n",
        "        x_t: a noisy version of the data for a given timestep\n",
        "    \"\"\"\n",
        "    x_t = alpha_bar[t,None].sqrt()*x_0 + (1.-alpha_bar[t,None]).sqrt()*eps\n",
        "    return x_t\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(20, 4))\n",
        "for i, t in enumerate([0, 6, 12, 25, 50]):\n",
        "    dataset, time_step = x_series[t], t\n",
        "    figure.add_subplot(1,5,i+1)\n",
        "    plt.title(time_step)\n",
        "    plt.axis(\"off\")\n",
        "    eps = torch.randn_like(x_0)\n",
        "    x_t = sample_x_t(x_0, t,alpha_bar, eps)\n",
        "    plt.scatter(x_t[:,0],x_t[:,1],s=15,alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEX8iUOutDKK"
      },
      "source": [
        "## Training\n",
        "We will now train a denoising model to learn the reverse process.  \n",
        "First, we need to create a dataset with our data.  \n",
        "### Dataset\n",
        "We now define a torch dataset to load our data.  We then split the data into a train and test set and create dataloaders for each.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmNBG87EtDKK"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "class DinoDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx].float()\n",
        "\n",
        "dataset = DinoDataset(x_0)\n",
        "\n",
        "# Train/Test Split\n",
        "train_size = int(0.9 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtPGITp5gT50"
      },
      "source": [
        "### Model:\n",
        "Now that the dataset and dataloaders are created, we can define the model.  \n",
        "We will use a simple MLP with 5 layers of 64 neurons each and a final layer to output the predicted noise.\n",
        "We will use the GELU activation function between each layer.  \n",
        "Remember that the output of the model will be the predicted noise which has the same dimension as the input.\n",
        "Complete the following class to define the model.  \n",
        "Since we will be training the model on the data for different timesteps, we will need to pass the timestep as an additional input to the model.  \n",
        "We will do this by concatenating the timestep to the input data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqG9nL4MtDKK"
      },
      "outputs": [],
      "source": [
        "class Denoisier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(3,64),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(64,64),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(64,64),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(64,64),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(64,2)\n",
        "        )\n",
        "    def forward(self, x:torch.Tensor, t:torch.Tensor) -> torch.Tensor:\n",
        "        # Concatenate the timestep to the input data\n",
        "        x = torch.cat((x,t.reshape(-1,1)),dim=1)\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdZz4uzugmQ6"
      },
      "source": [
        "### Training loop:\n",
        "At this point, we have all the components to train the model.  \n",
        "The training loop of a denoising model is actually quite simple.  \n",
        "Look at the training algorithm from the paper and implement the training loop.  \n",
        "![training_loop](images/training.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_bh_QyutDKK"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm # you can use tqdm to display a progress bar\n",
        "\n",
        "def train(model:torch.nn.Module, train_dataloader:DataLoader, optimizer:torch.optim.Optimizer, alpha_bar:torch.Tensor, epochs:int=50, device:str='cpu'):\n",
        "    # don't forget to move everything to the correct device\n",
        "    progress_bar = tqdm(range(epochs),desc=\"Training\")\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x in train_dataloader:\n",
        "            x = x.to(device)\n",
        "            eps = torch.randn_like(x)\n",
        "            t = torch.randint(T,(x.shape[0],), device=device)\n",
        "            x_t = sample_x_t(x,t,alpha_bar.to(device),eps)\n",
        "            eps_pred = model(x_t,t)\n",
        "            loss = torch.nn.functional.mse_loss(eps_pred,eps)\n",
        "            total_loss += loss.item()*x.shape[0]\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        progress_bar.set_postfix({\"epoch\": epoch, \"loss\": total_loss/x.shape[0]})\n",
        "        progress_bar.update()\n",
        "    progress_bar.close()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gyOSMWptDKK"
      },
      "source": [
        "### Training:\n",
        "Now instantiate the model and optimizer (Adam with a learning rate of 1e-3) and train the model for 3000 epochs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ff0ec0705d0b4b899c27405e49777904",
            "e28e0b941950447a98da71efeff9f1c2",
            "8a97e77fb5cd4b4aae8ead106ea700c6",
            "2b977a780d6b47a691699be9254c576c",
            "44ff2d36b1ac4dd2b699d91df263c925",
            "5df7af6dc686422bbde6a1366d37b4f1",
            "043d01bcd99641fda43e813ba2efe7e8",
            "fa5f340489b94fb8acbdbf7ef874e571",
            "971de86e5d6f4f30827ea8e66ab3dcf2",
            "fef32e50cebd439995b6dd3213e62da4",
            "9306690fc9a6405b85be4fc0b86b85db"
          ]
        },
        "id": "r2EveO0otDKK",
        "outputId": "221675bf-eade-45ca-a133-ef97c630d079"
      },
      "outputs": [],
      "source": [
        "model = Denoisier().to(device) # don't forget to move the model to the correct device\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "train(model, train_loader, optimizer, alpha_bar, epochs=3000, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4jRNgL5tDKK"
      },
      "source": [
        "### Sampling:\n",
        "Here is the moment of truth!\n",
        "We will now sample from the model and see if we did manage to learn the reverse process.  \n",
        "Remember that the sampling algorithm is the following:  \n",
        "![](images/sampling.png)  \n",
        "Use it to complete the following function.  Once this is done, use it to sample 1000 points and plot them.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "J_xLK18MhMbF",
        "outputId": "1d951f1b-31cb-4529-92c3-9dd698d163c5"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample(num_samples:int, model:torch.nn.Module, alpha:torch.Tensor, alpha_bar:torch.Tensor, T:int=200, device:str='cpu') -> List[torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Takes the model, the alphas and the number of timesteps and returns a list of the sampled data for each timestep\n",
        "    args:\n",
        "        model: the denoising model\n",
        "        alphas: the alphas of the noise schedule\n",
        "        T: the number of timesteps\n",
        "    returns:\n",
        "        x_series: a list of the sampled data for each timestep\n",
        "    \"\"\"\n",
        "    alpha = alpha.to(device)\n",
        "    alpha_bar = alpha_bar.to(device)\n",
        "    x_series = []\n",
        "    xt = torch.randn((num_samples,2)).to(device)\n",
        "    for t in reversed(range(T)):\n",
        "        t_batch = torch.full((num_samples,), t).to(device)\n",
        "        noise_pred = model(xt, t_batch)\n",
        "        mu_hat_t = (xt - (1-alpha[t,None])/(1-alpha_bar[t,None]).sqrt()*noise_pred)/(alpha[t,None]).sqrt()\n",
        "\n",
        "        z = torch.randn_like(xt).to(device)\n",
        "        sigma = (1.-alpha[t]).sqrt()\n",
        "        xt = mu_hat_t + sigma*z\n",
        "        x_series.append(xt.clone().detach().to('cpu'))  # move the data to the cpu before appending\n",
        "    return x_series\n",
        "\n",
        "steps = sample(1000, model, alphas, alpha_bar, T=T, device=device)\n",
        "plt.figure(figsize=[6, 6])\n",
        "plt.scatter(steps[-1][:,0],steps[-1][:,1],s=15,alpha=0.5)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uff1TtQwtDKL"
      },
      "source": [
        "We are getting there! But we can do better. Even if the model would probably have converged if we trained it for longer, for now let's try to train faster and better.  \n",
        "### Time embedding:\n",
        "We will now try to improve the model by adding a time embedding.  \n",
        "This is a common technique in diffusion models to help the model learn the temporal aspect of the data.  \n",
        "We will use a simple sinusoidal position embedding.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjNKqfQMiOht"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class SinusoidalPositionEmbeddings(torch.nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm1P9xxytDKL"
      },
      "source": [
        "Now, complete the following class to define the model with the time embedding and train it for 2000 epochs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "247ba8c2ecfa44eba4e1f7bf6cdb6de4",
            "a8652cf0f50a4dd2944b6c00589b53cf",
            "02aeb15c28914b9f9ea0ce0fe8126101",
            "a7840a61b55d4da4baace57bdade9e4a",
            "8c7bfb6885b54e208a2660833ae14113",
            "b3ed871edad94604ac0cb796128439ef",
            "2b36902929ce4e5b8f9eddf96befe381",
            "28a99c22e63f4222979799356da5363e",
            "0584c8119c5547e498223b3f3fe1b86e",
            "f52e80b428ce40f3b5dc633b73750803",
            "d09e519aeb1c475da96550a03a9dbdd5"
          ]
        },
        "id": "EMIpf9ZNtDKL",
        "outputId": "c25b3cce-3b4a-4038-cb42-fb826a424d92"
      },
      "outputs": [],
      "source": [
        "class DenoisierWithTimeEmbedding(torch.nn.Module):\n",
        "    def __init__(self, t_emb_dim:int=32, device:str='cpu'):\n",
        "        super().__init__()\n",
        "        self.time_embedder = SinusoidalPositionEmbeddings(t_emb_dim)\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2 + t_emb_dim,64),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(64,64),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(64,64),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(64,64),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(64,2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t_emb = self.time_embedder(t)\n",
        "        x = torch.cat((x,t_emb), dim=1)\n",
        "        return self.layers(x)\n",
        "\n",
        "model = DenoisierWithTimeEmbedding().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "train(model, train_loader, optimizer, alpha_bar, epochs=3000, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RF1W7qZtDKL"
      },
      "source": [
        "Sample 1000 new points and plot them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "REMJ9OivtDKL",
        "outputId": "2cbfa28f-a18e-4def-f662-2fb1b4a8c3f3"
      },
      "outputs": [],
      "source": [
        "steps = sample(1000, model, alphas, alpha_bar, T=T, device=device)\n",
        "plt.figure(figsize=[6, 6])\n",
        "plt.scatter(steps[-1][:,0],steps[-1][:,1],s=15,alpha=0.5)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP-5Wax_tDKL"
      },
      "source": [
        "Using the previous code, make a small animation to visualize the sampling process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "id": "YZeY4jKltDKL",
        "outputId": "b1bde434-3f98-4aeb-b99e-820f720206f5"
      },
      "outputs": [],
      "source": [
        "animate_backward = partial(animate, series=steps)\n",
        "\n",
        "anim = FuncAnimation(fig, animate_forward, frames=len(x_series),\n",
        "                    interval=250)  # 500ms between frames\n",
        "\n",
        "\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMFQ5tFOtDKL"
      },
      "source": [
        "## Image models\n",
        "In the previous part, we have seen how to train an unconditional diffusion model to generate samples from a target distribution.  \n",
        "We did it on a 2d example, but in practice, diffusion models are often used to generate more structured data like images or audio.  \n",
        "In this part, we will see how to use a pretrained text2image model to generate images from text.  \n",
        "We will use the [diffusers](https://github.com/huggingface/diffusers) library to load a pretrained model and generate images of faces.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17lhd3CStDKM",
        "outputId": "37ff1488-dc72-4ad6-cbac-3d5698d6b21f"
      },
      "outputs": [],
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "repo_id = \"google/ddpm-celebahq-256\" # \"google/ddpm-church-256\"\n",
        "model = UNet2DModel.from_pretrained(repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldpwlrwptDKM"
      },
      "source": [
        "The architecture of the neural network, referred to as **model**, commonly follows the UNet architecture as proposed in [this paper](https://arxiv.org/abs/1505.04597) and improved upon in the Pixel++ paper.\n",
        "\n",
        "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unet-model.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuAO5FUitDKM"
      },
      "source": [
        "We can start by loading the model and checking its configuration in particular to know what is its input shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41PXehPYtDKM",
        "outputId": "4ac93f81-6b93-455d-c7b3-a15fdc84363f"
      },
      "outputs": [],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRXNDK37tDKM"
      },
      "source": [
        "Generate a random input for the model of shape (1, 3, 256, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuQJt3istDKM",
        "outputId": "00be6735-f8ae-4f09-f5c9-e75af8815c45"
      },
      "outputs": [],
      "source": [
        "noisy_sample = torch.randn(...)\n",
        "noisy_sample.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mc6Ie8ZtDKM"
      },
      "source": [
        "The following code does a noise prediction at for a given timestep and a given noisy image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOZseAmktDKM",
        "outputId": "0de21f99-75f9-4569-fd10-5a112b00a317"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    noisy_residual = model(sample=noisy_sample, timestep=2).sample\n",
        "noisy_residual.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYx0UkhStDKM"
      },
      "source": [
        "diffusers pretrained models come with a scheduler, responsible for the noise schedule.  \n",
        "Here is the corresponding scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCU70J44tDKM",
        "outputId": "0a03d000-3b59-47eb-8207-e8d59c400d7e"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMScheduler\n",
        "\n",
        "scheduler = DDPMScheduler.from_config(repo_id)\n",
        "scheduler.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UwjbklTtDKM"
      },
      "source": [
        "Knowing that $\\bar{\\alpha_t}$ is the cumulative product of the alphas where $\\alpha = 1 - \\beta$, we can compute the $\\bar{\\alpha_t}$ and plot them.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "Lx3fid7ptDKM",
        "outputId": "27120403-b2cb-4b35-a85d-37a3e31ec968"
      },
      "outputs": [],
      "source": [
        "T=1000\n",
        "beta_min=0.0001\n",
        "beta_max=0.02\n",
        "betas = torch.linspace(beta_min, beta_max, T)\n",
        "alphas = 1. - betas\n",
        "alpha_bar = torch.cumprod(alphas, dim=0)\n",
        "plt.figure(figsize=[6, 6])\n",
        "plt.plot(torch.arange(T), alpha_bar)\n",
        "plt.xlabel('Timestep')\n",
        "plt.ylim(0,1.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s0sO7JbtDKM"
      },
      "source": [
        "Let's visualize the noise schedule applied to an image.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahi1uypCtDKM",
        "outputId": "ffe36227-daf3-4257-b1bd-86bcde6f5d3c"
      },
      "outputs": [],
      "source": [
        "!wget https://efrosgans.eecs.berkeley.edu/SwappingAutoencoder/results_for_paper_with_new_ffhq2/ffhq/input_structure/00001__000.png -O image.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "EwAnkusStDKM",
        "outputId": "78b87b1f-006e-4383-a2c7-fe5124392a3a"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "x_0 = Image.open(\"image.jpg\")\n",
        "\n",
        "mean = [ 0.485, 0.456, 0.406 ]\n",
        "std = [ 0.229, 0.224, 0.225 ]\n",
        "normalize = transforms.Normalize(mean, std)\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean= [-m/s for m, s in zip(mean, std)],\n",
        "   std= [1/s for s in std]\n",
        ")\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((256, 256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                normalize])\n",
        "x_0 = transform(x_0)\n",
        "\n",
        "def tensor_to_img(img):\n",
        "    if len(img.shape) == 4:\n",
        "        img = img.squeeze(0)\n",
        "    img = inv_normalize(img)\n",
        "    img = img.permute(1, 2, 0)\n",
        "    return img\n",
        "\n",
        "img = tensor_to_img(x_0)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhp8HEgBtDKN"
      },
      "source": [
        "We need to modify a little bit the sampling function to make it compatible with image inputs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZOpDSKyVtDKN",
        "outputId": "2e203dbe-9b3d-42d3-a5bd-35e9975c1fc1"
      },
      "outputs": [],
      "source": [
        "def sample_x_t(x_0, t, alpha_bar, eps):\n",
        "    return alpha_bar[t, None, None, None].sqrt()*x_0 + (1.-alpha_bar[t, None, None , None]).sqrt()*eps\n",
        "\n",
        "eps = torch.randn_like(x_0)\n",
        "t = 500\n",
        "x_t = sample_x_t(x_0, t, alpha_bar, eps)\n",
        "img = tensor_to_img(x_t)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHZmwR5ytDKN"
      },
      "source": [
        "Look at the mean and std of the image of a noisysample at time step 999. What would you expect and is it what you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyzCa3NGtDKN",
        "outputId": "f22e2d05-5667-4a73-b29e-5256e44539fc"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the evolution of the image at several timestep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "XRABVUMItDKN",
        "outputId": "f50d4f07-8f99-460a-e928-3b3872d0d275"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[12, 6])\n",
        "for i, t in enumerate([0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 999]):\n",
        "    eps = torch.randn_like(x_0)\n",
        "    x_t = sample_x_t(x_0, t, alpha_bar, eps)\n",
        "    plt.subplot(1, 11, i+1)\n",
        "    plt.imshow(tensor_to_img(x_t))\n",
        "    plt.axis('off')\n",
        "    #tight layout\n",
        "    plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xe3zIKftDKN",
        "outputId": "8c4e3eb7-e82e-407d-a794-03920546708d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "import torch\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.axis('off')\n",
        "\n",
        "time_steps = [0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 999]\n",
        "all_frames = []\n",
        "\n",
        "for t in time_steps:\n",
        "    eps = torch.randn_like(x_0)\n",
        "    x_t = sample_x_t(x_0, t, alpha_bar, eps)\n",
        "    noisy_img = inv_normalize(x_t)\n",
        "    noisy_img = noisy_img.permute(1, 2, 0)\n",
        "    all_frames.append(noisy_img.detach().cpu().numpy())\n",
        "\n",
        "def update(frame):\n",
        "    ax.clear()\n",
        "    ax.imshow(all_frames[frame])\n",
        "    ax.set_title(f't = {time_steps[frame]}')\n",
        "    ax.axis('off')\n",
        "    return ax,\n",
        "\n",
        "anim = animation.FuncAnimation(\n",
        "    fig,\n",
        "    update,\n",
        "    frames=len(time_steps),\n",
        "    interval=200,\n",
        "    blit=False,\n",
        "    repeat=True,\n",
        "    repeat_delay=1000\n",
        ")\n",
        "\n",
        "plt.close()\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHeoXrs2tDKN"
      },
      "source": [
        "Let's now try to generate an image from a text prompt given the noise schedule previously computed and our previous sampling function.  \n",
        "Let's define a new sampling function which is compatible with `diffusers` unet. No need here to store the intermediate steps.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzmuH4xbtDKN"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "\n",
        "model = model.to(device)\n",
        "@torch.no_grad()\n",
        "def sample(num_samples, model, alpha, alpha_bar, T=200, device='cuda'):\n",
        "    alpha = alpha.to(device)\n",
        "    alpha_bar = alpha_bar.to(device)\n",
        "    xt = ... .to(device)\n",
        "    for t in ...:\n",
        "        t_batch = ... .to(device)\n",
        "        noise_pred = model(xt, t_batch).sample\n",
        "        mu_hat_t = (xt - (1-alpha[t,None])/(1-alpha_bar[t,None]).sqrt()*noise_pred)/(alpha[t,None]).sqrt()\n",
        "\n",
        "        z = ... .to(device)\n",
        "        sigma = ...\n",
        "        xt = ...\n",
        "    return xt\n",
        "\n",
        "x_0 = sample(1, model, alphas, alpha_bar, T=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the generated image.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH26CIdqWN8H"
      },
      "outputs": [],
      "source": [
        "img = tensor_to_img(x_0[0].cpu())\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQC5H3QmtDKN"
      },
      "source": [
        "This is it for this practical session.  I hope you gained more intuition about diffusion models.  \n",
        "In the next session we will work with conditional diffusion models. If you finish early, you can try to run some text2image models using Fooocus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArKC0LclUj2I"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgRLFNoMtDKN",
        "outputId": "48832dfb-c93f-4591-ecaa-54a5fa3174b8"
      },
      "outputs": [],
      "source": [
        "!pip install pygit2==1.15.1\n",
        "%cd /content\n",
        "!git clone https://github.com/lllyasviel/Fooocus.git\n",
        "%cd /content/Fooocus\n",
        "!python entry_with_update.py --share --always-high-vram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MwJkmtlWdLv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.12 (AIFtorch)",
      "language": "python",
      "name": "aiftorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02aeb15c28914b9f9ea0ce0fe8126101": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28a99c22e63f4222979799356da5363e",
            "max": 3000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0584c8119c5547e498223b3f3fe1b86e",
            "value": 3000
          }
        },
        "043d01bcd99641fda43e813ba2efe7e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0584c8119c5547e498223b3f3fe1b86e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "247ba8c2ecfa44eba4e1f7bf6cdb6de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8652cf0f50a4dd2944b6c00589b53cf",
              "IPY_MODEL_02aeb15c28914b9f9ea0ce0fe8126101",
              "IPY_MODEL_a7840a61b55d4da4baace57bdade9e4a"
            ],
            "layout": "IPY_MODEL_8c7bfb6885b54e208a2660833ae14113"
          }
        },
        "28a99c22e63f4222979799356da5363e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b36902929ce4e5b8f9eddf96befe381": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b977a780d6b47a691699be9254c576c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fef32e50cebd439995b6dd3213e62da4",
            "placeholder": "​",
            "style": "IPY_MODEL_9306690fc9a6405b85be4fc0b86b85db",
            "value": " 3000/3000 [01:27&lt;00:00, 38.83it/s, epoch=2999, loss=2.05]"
          }
        },
        "44ff2d36b1ac4dd2b699d91df263c925": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5df7af6dc686422bbde6a1366d37b4f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a97e77fb5cd4b4aae8ead106ea700c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa5f340489b94fb8acbdbf7ef874e571",
            "max": 3000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_971de86e5d6f4f30827ea8e66ab3dcf2",
            "value": 3000
          }
        },
        "8c7bfb6885b54e208a2660833ae14113": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9306690fc9a6405b85be4fc0b86b85db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "971de86e5d6f4f30827ea8e66ab3dcf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7840a61b55d4da4baace57bdade9e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f52e80b428ce40f3b5dc633b73750803",
            "placeholder": "​",
            "style": "IPY_MODEL_d09e519aeb1c475da96550a03a9dbdd5",
            "value": " 3000/3000 [01:20&lt;00:00, 41.90it/s, epoch=2999, loss=2.15]"
          }
        },
        "a8652cf0f50a4dd2944b6c00589b53cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3ed871edad94604ac0cb796128439ef",
            "placeholder": "​",
            "style": "IPY_MODEL_2b36902929ce4e5b8f9eddf96befe381",
            "value": "Training: 100%"
          }
        },
        "b3ed871edad94604ac0cb796128439ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d09e519aeb1c475da96550a03a9dbdd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e28e0b941950447a98da71efeff9f1c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5df7af6dc686422bbde6a1366d37b4f1",
            "placeholder": "​",
            "style": "IPY_MODEL_043d01bcd99641fda43e813ba2efe7e8",
            "value": "Training: 100%"
          }
        },
        "f52e80b428ce40f3b5dc633b73750803": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa5f340489b94fb8acbdbf7ef874e571": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fef32e50cebd439995b6dd3213e62da4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff0ec0705d0b4b899c27405e49777904": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e28e0b941950447a98da71efeff9f1c2",
              "IPY_MODEL_8a97e77fb5cd4b4aae8ead106ea700c6",
              "IPY_MODEL_2b977a780d6b47a691699be9254c576c"
            ],
            "layout": "IPY_MODEL_44ff2d36b1ac4dd2b699d91df263c925"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
