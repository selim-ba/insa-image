{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47337546"
      },
      "source": [
        "# Algorithm Unrolling for Image Restoration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "450402ce"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/esoubies/TP_INSA/blob/master/TP_2_Image_PnP_Unroll.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88703323"
      },
      "source": [
        "This practical session is dedicated to unrolling an algorithm.\n",
        "They will be illustrated on deblurring problems using deepinv library.\n",
        "\n",
        "The outline of the session is:\n",
        "1. Coding a blur model and testing a pretrained denoiser **(Same as TP1)**\n",
        "2. Implementing a PnP HQS in a `nn.Module`\n",
        "3. Optimize the step-size and denoiser strength parameters of the unroll HQS on a single image\n",
        "3. Train the step-size and denoiser strength parameters of the unroll HQS on a dataset\n",
        "\n",
        "\n",
        "Reminders from TP1:\n",
        "1. In torch the image format is B x C x H x W where\n",
        "  - B is the batch size\n",
        "  - C is the number of channels (C=1 for grayscale C = 3 for RGB)\n",
        "  - H is the number of pixels in the vertical direction\n",
        "  - W is the number of pixels in the horizontal direction\n",
        "2. To take advantage of the GPU acceleration, make sure to pass the `device` variable to the torch functions. For example,\n",
        "`x = torch.zeros((1,1,25,25))` creates an image of size (1,1,25,25) on the cpu than can be transfered to the GPU using `x = x.to('cuda')`.\n",
        "The image can directly by created on the GPU using `x = torch.zeros((1,1,25,25), device = 'cuda')`\n",
        "In the following code block, the variable `device` is created to easily swith from CPU to GPU. You can use it in your code instead of hard coding 'cpu' 'cuda'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "93afa4ec"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.fft import fft2, ifft2, fftshift, ifftshift\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "!pip install git+https://github.com/deepinv/deepinv.git\n",
        "import deepinv as dinv\n",
        "from deepinv.physics.generator import MotionBlurGenerator\n",
        "\n",
        "psnr = dinv.loss.PSNR()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f307d160"
      },
      "source": [
        "# Part 1: Blur model and pretrained denoiser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ae0fa5a"
      },
      "source": [
        "## Test a pre-learned denoiser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e422d176"
      },
      "source": [
        "In this section, we load an image and pretrained denoiser from the deepinv library. The denoiser is tested on a denoising problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "360daab2"
      },
      "outputs": [],
      "source": [
        "## Open the image\n",
        "url = dinv.utils.demo.get_image_url(\"butterfly.png\") # Other images are available here : https://huggingface.co/datasets/deepinv/images\n",
        "x0 = dinv.utils.demo.load_url_image(url, grayscale=False).to(device)\n",
        "x0 = x0[:,:,::2,::2] # subsampling to reduce memory usage in training of the unrolled network\n",
        "\n",
        "# Plot image\n",
        "dinv.utils.plot(x0,'Input Image',figsize=(3,3))\n",
        "print(x0.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CqArrYzkVxA"
      },
      "source": [
        "There are three possible denoisers in deepinv: DRUnet, DnCNN, TV.\n",
        "You can select one by commenting/decommenting the appropriate lines and play with the three during the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qamf7HmXkKMG"
      },
      "outputs": [],
      "source": [
        "# Load the DRUNet denoiser\n",
        "# https://deepinv.github.io/deepinv/stubs/deepinv.models.DRUNet.html\n",
        "D = dinv.models.DRUNet(pretrained='download').to(device)\n",
        "\n",
        "# Load the DnCNN denoiser (WARNING: the proposed weights are only trained for noise level sigma = 2/255)\n",
        "# https://deepinv.github.io/deepinv/stubs/deepinv.models.DnCNN.html\n",
        "#D = dinv.models.DnCNN(pretrained='download').to(device)\n",
        "\n",
        "# TV denoiser (only in last version of deepinv)\n",
        "# Dtv = dinv.models.TVDenoiser().to(device)\n",
        "# def D(x,sigma):\n",
        "#   return Dtv(x,ths=2*sigma**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYL7-Boj2ZO"
      },
      "source": [
        "**Compute** a noisy image\n",
        "$$ y = x_0 + \\xi $$\n",
        "where $\\xi \\sim \\mathcal{N}(0,\\sigma^2 \\mathsf{Id})$.\n",
        "\n",
        "Denoise the image $y$ by using a pre-learned denoiser for different level of noise $\\sigma^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4448cdb4",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "sigma = 4/255 # in [0, 0.2]\n",
        "y = x0 + sigma*torch.randn_like(x0,device=device)\n",
        "\n",
        "Dy = D(y,sigma=sigma)\n",
        "dinv.utils.plot([y,Dy],['Noisy  (%.2f dB)'%psnr(x0,y),'Denoised (%.2f dB)'%psnr(x0,Dy)],figsize=(5,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pq_Q2pki46V"
      },
      "source": [
        "## Blur model\n",
        "\n",
        "This section generates a motion blur kernel from the deepinv library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "533cf4d6",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "## Generate a motion blur\n",
        "torch.manual_seed(1)  # fix random seed for reproducibility\n",
        "generator = MotionBlurGenerator((25, 25), num_channels=1, sigma = 0.5)\n",
        "blur = generator.step(seed = 1)\n",
        "kt = blur['filter']\n",
        "\n",
        "# Embed the kernel in a MxNx3 image, and put center at pixel (0,0)\n",
        "k = torch.zeros_like(x0)\n",
        "m,n = kt.shape[-2:]\n",
        "k[:,:,0:m,0:n] = kt/torch.sum(kt)\n",
        "k = torch.roll(k,(-int(m/2),-int(n/2)),(-2,-1))\n",
        "\n",
        "# Display kernel\n",
        "dinv.utils.plot([kt, fftshift(k,dim=(-2,-1))],['Blur kernel [25 x 25]','Blur kernel [M N]'],figsize=(5,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e3c8763"
      },
      "source": [
        "The forward model is:\n",
        "$$ y = k \\star x_0 + \\xi $$\n",
        "where $\\xi \\sim \\mathcal{N}(0,\\sigma^2 \\mathsf{Id})$ and $k$ is the generated blur kernel.\n",
        "\n",
        "Implement the code generating $y$ using the Fast Fourier Transform. (You can look at the documentation of `torch.fft.fft2`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de80a50d"
      },
      "outputs": [],
      "source": [
        "sigma = 0.1#2/255  # noise level\n",
        "\n",
        "# Draw a sample of the direct model for image deblurring (apply blur and add Gaussian noise)\n",
        "fk = fft2(k) # à coder\n",
        "y0 = torch.real(ifft2( fft2(x0) * fk ))\n",
        "y = y0 + sigma*torch.randn_like(x0,device=device)\n",
        "\n",
        "dinv.utils.plot(y,'Blur and noisy image',figsize=(3,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlzUBEH1_Rut"
      },
      "source": [
        "## Part 2: Implemeting PnP HQS in a nn.Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqR4QL3N_te2"
      },
      "source": [
        "The goal of this part is to implement a PnP-HQS in a class `HQS` that inherits from `nn.Module`. This will allow to use the pytorch API to optimize the parameters with respect to some criteria.\n",
        "\n",
        "We recall that the HQS iteration is given by :\n",
        "$$ x_{k+1} = D_{\\eta} \\circ \\mathsf{Prox}_{\\tau f} (x_k) $$\n",
        "where\n",
        "- $f$ is the data fidelity term defined by $f(x) = \\frac{1}{2} \\|k \\star x - y\\|_2^2$.\n",
        "- $D_\\eta$ is the denoiser of strength $\\eta$\n",
        "- $\\tau$ is the penalty parameter\n",
        "\n",
        "The HQS class should contain two methods :\n",
        "- the `__init__` method with parameters :\n",
        "  - fk: the fourier transform of the convolution kernel\n",
        "  - D: the denoiser\n",
        "  - niter: the number of unrolled iterations\n",
        "  - init_tau: the initial value of the penalty parameter $\\tau$  \n",
        "  - init_s: as we know that the denoiser strength $\\eta$ should be adapted to the noise level $\\sigma$, we consider a linear relationship $\\eta = s \\sigma$ where the scaling $s$ will be adjusted, initialized with init_s.  \n",
        "- the `forward` method that implements the unrolled HQS. It returns the `niter`-th iterates of the HQS algorithm form the initial point $x_0$. In takes as inputs:\n",
        "  - `y` the data\n",
        "  - `sigma` the noise level $\\sigma$\n",
        "\n",
        "**Todo:** Implement the class. To specify which parameters are to be learned in the training phase, the class `nn.parameter.Parameter` can be used. Moreover, the parameters of the denoiser should be set to non-trainable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rLkUOuB_Ql9"
      },
      "outputs": [],
      "source": [
        "class HQS(nn.Module):\n",
        "    def __init__(self,fk, D, niter=10, init_tau = 1, init_s = 0.1 ):\n",
        "        super(HQS, self).__init__() ## needed to call the constructor of the parent class\n",
        "\n",
        "        self.niter = # storing the number of unrolled iterations in the class\n",
        "        self.D = # storing the denoiser in the class\n",
        "        self.fk = # storing the Fourier transform of the convolution kernel in the class\n",
        "\n",
        "        # Set the denoiser parameters to non-trainable\n",
        "        for param in self.D.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "        # code\n",
        "    def forward(self, y, sigma):\n",
        "        # code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9XZglCD_Q5X"
      },
      "source": [
        "**Todo:** Using this class, deblur the image using 10 unrolled iterations. Try to find \"good\" parameters $s$ and $\\tau$ by hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzCnbeNmD7Ug"
      },
      "outputs": [],
      "source": [
        "hqs_init = HQS(fk, D, niter = 10, init_tau = , init_s = )\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_reconstructed_init = hqs_init(y, sigma)\n",
        "    dinv.utils.plot([x0,y,x_reconstructed_init], ['Original', 'Degraded  (%.2f dB)'%psnr(x0,y),'Rec (%.2f dB)'%psnr(x0,x_reconstructed_init)],figsize=(8,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMTznFFV-RAk"
      },
      "source": [
        "## Part 3: Optimize HQS parameters on a single image\n",
        "\n",
        "As you may have noticed, adjusting the HQS parameters can be tricky.\n",
        "In this section, we propose to optimize the parameters $s$ and $\\tau$ so as to get the best deblurred image. In other words, we want to solve the following optimization problem.\n",
        "$$(̂\\hat{s},\\hat{\\tau}) = \\mathrm{arg \\, min}_{s,\\tau} \\|\\mathrm{HQS}_{s,\\tau}(y,\\sigma) - x_0\\|^2_2$$\n",
        "where $x_0$ is the true image and $y$ the blurred and noisy image.\n",
        "\n",
        "**Todo:** Implement the optimization algorithm. You can for example use:\n",
        "- the Adam optimizer\n",
        "- a learning_rate `1e-2`\n",
        "- a number of iteration `200`\n",
        "\n",
        "The following code contains a skeleton for this step. The end displays the images reconstructed using the HQS with initial parameters and with the optimzed ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcrQr2rbCNsV"
      },
      "outputs": [],
      "source": [
        "# Initialize the HQS algo\n",
        "hqs_opt_single = # use this name for subsequent comparisons\n",
        "\n",
        "# Optimizer and its parameters\n",
        "learning_rate =\n",
        "max_iter =\n",
        "optimizer =\n",
        "\n",
        "# Main loop to be implemented\n",
        "\n",
        "# Print optimized parameters\n",
        "print('Optimized parameters: tau = %f / s = %f' % (hqs_opt_single.tau[0], hqs_opt_single.s[0],))\n",
        "\n",
        "# Display deconvolved image\n",
        "with torch.no_grad():\n",
        "    x_reconstructed_init = hqs_init(y, sigma)\n",
        "    x_reconstructed_single = hqs_opt_single(y, sigma)\n",
        "    dinv.utils.plot([x0,y,x_reconstructed_init,x_reconstructed_single], ['Original', 'Degraded  (%.2f dB)'%psnr(x0,y),'Rec (Init) (%.2f dB)'%psnr(x0,x_reconstructed_init),'Rec (Single) (%.2f dB)'%psnr(x0,x_reconstructed_single)],figsize=(12,12))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53W2_LyHE01l"
      },
      "source": [
        "## Part 4: Train the HQS parameters on a dataset\n",
        "\n",
        "In practice, we do not have access to the true image $x_0$ and so, the previous way of adjusting the parameters is unrealistic. Moreover, the obtained parameters that are maximizing the performance on this single image may not lead to good result on a different image / noise level.\n",
        "\n",
        "In this section, we will train the $\\tau$ and $s$ parameters of the HQS unrolled algorithm using a dataset.\n",
        "\n",
        "We first load the dataset and creates the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNWRT5m6GXtn"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\".\")\n",
        "ORIGINAL_DATA_DIR = BASE_DIR / \"datasets\"\n",
        "\n",
        "img_size = x0.shape[-1]\n",
        "\n",
        "# For simplicity, we use a small dataset for training.\n",
        "# To be replaced for optimal results. For example, you can use the larger \"drunet\" dataset.\n",
        "train_dataset_name = \"CBSD500\"\n",
        "test_dataset_name = \"set3c\"\n",
        "# Specify the  train and test transforms to be applied to the input images.\n",
        "test_transform = transforms.Compose(\n",
        "    [transforms.CenterCrop(img_size), transforms.ToTensor()]\n",
        ")\n",
        "train_transform = transforms.Compose(\n",
        "    [transforms.RandomCrop(img_size), transforms.ToTensor()]\n",
        ")\n",
        "# Define the base train and test datasets of clean images.\n",
        "train_base_dataset = dinv.utils.demo.load_dataset(\n",
        "    train_dataset_name, data_dir=ORIGINAL_DATA_DIR, transform=train_transform\n",
        ")\n",
        "test_base_dataset = dinv.utils.demo.load_dataset(\n",
        "    test_dataset_name, data_dir=ORIGINAL_DATA_DIR, transform=test_transform\n",
        ")\n",
        "\n",
        "num_workers = 2 if torch.cuda.is_available() else 0\n",
        "train_batch_size = 8 # adjust to memory capacities\n",
        "test_batch_size = 1\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_base_dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_base_dataset, batch_size=test_batch_size, num_workers=num_workers, shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucfn-z8aGHT1"
      },
      "source": [
        "Let first try the unrolled HQS with the previously optimized parameters on differents images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Gs3z-CpIj0g"
      },
      "outputs": [],
      "source": [
        "sigma_test = 0.1\n",
        "\n",
        "for i,data in enumerate(test_dataloader):\n",
        "  xtest = data[0].unsqueeze(0).to(device)\n",
        "  ytest = torch.real(ifft2( fft2(xtest) * fk )) + sigma_test*torch.randn_like(xtest,device=device)\n",
        "  with torch.no_grad():\n",
        "    x_reconstructed_init = hqs_init(ytest, sigma_test)\n",
        "    x_reconstructed_single = hqs_opt_single(ytest, sigma_test)\n",
        "    dinv.utils.plot([xtest,ytest,x_reconstructed_init,x_reconstructed_single], ['Original', 'Degraded  (%.2f dB)'%psnr(xtest,ytest),'Rec (Init) (%.2f dB)'%psnr(xtest,x_reconstructed_init),'Rec (Single) (%.2f dB)'%psnr(xtest,x_reconstructed_single)],figsize=(12,12))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8vil-J5Jp1S"
      },
      "source": [
        "Now let's train the parameters $(s, \\tau)$ on a dataset.\n",
        "\n",
        "In other words, we want to solve the following optimization problem :\n",
        "$$(\\hat{s},\\hat{\\tau}) = \\mathrm{arg \\, min}_{s,\\tau} \\frac{1}{n} \\sum_{i=1}^n \\|\\mathrm{HQS}_{s,\\tau}(y_i,\\sigma_i) - x_i\\|^2_2$$\n",
        "where:\n",
        "- $n$ is the number of images in the dataset\n",
        "- $x_i$ are picked from the dataset\n",
        "- $\\sigma_i$ is a noise level. It can either be fixed at a prescribed value or randomly picked in $[0,0.2]$.\n",
        "- $y_i$ is the noisy image that is $y_i = x_i + \\mathcal{N}(0,\\sigma_i \\textrm{Id})$\n",
        "\n",
        "**Todo:** Implement the loop training the parameters $(s, \\tau)$ of the HQS. You can use a Adam optimizer. Depending on your implementation, adapt the learning rate. Running the training will be long so you can at first use 2 epochs.\n",
        "The following code contains a skeleton of this task. The end displays the images reconstructed using the HQS with initial parameters and with the optimzed ones for the single image and on the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCL03d5UE_tF"
      },
      "outputs": [],
      "source": [
        "hqs_trained_dataset = # use this name for subsequent comparisons\n",
        "\n",
        "learning_rate =\n",
        "max_epoch =\n",
        "optimizer =\n",
        "\n",
        "# Main Loop\n",
        "\n",
        "# Print optimized parameters\n",
        "print('Optimized parameters: tau = %f / s = %f' % (hqs_trained_dataset.tau[0], hqs_trained_dataset.s[0]))\n",
        "\n",
        "# Display deconvolved image\n",
        "with torch.no_grad():\n",
        "    x_reconstructed_init = hqs_init(y, sigma)\n",
        "    x_reconstructed_single = hqs_opt_single(y, sigma)\n",
        "    x_reconstructed_dataset = hqs_trained_dataset(y, sigma)\n",
        "    dinv.utils.plot([x0,y,x_reconstructed_init,x_reconstructed_single,x_reconstructed_dataset], ['Original', 'Degraded  (%.2f dB)'%psnr(x0,y),'Rec (Init) (%.2f dB)'%psnr(x0,x_reconstructed_init),'Rec (Single) (%.2f dB)'%psnr(x0,x_reconstructed_single),'Rec (Dataset) (%.2f dB)'%psnr(x0,x_reconstructed_dataset)],figsize=(14,14))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Todo:** Test the HQS algorithms with the different parameters on the test set.\n",
        "You can for example :\n",
        "- test them on different noise level\n",
        "- train again the parameters in Part 4 for a specific noise level or randomly picked in a interval and test with different noise level (e.g. the noise level used in the training and other noise levels)\n",
        "- Implement an HQS with different parameters at each iteration i.e. $(s_k, \\tau_k)$.\n",
        "\n",
        "The following code is a starter to perform the comparisons."
      ],
      "metadata": {
        "id": "rkmkPUaJOohJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdzaiPDjMH8c"
      },
      "outputs": [],
      "source": [
        "sigma_test = 0.2\n",
        "\n",
        "for i,data in enumerate(test_dataloader):\n",
        "  xtest = data[0].unsqueeze(0).to(device)\n",
        "  ytest = torch.real(ifft2( fft2(xtest) * fk )) + sigma_test*torch.randn_like(xtest,device=device)\n",
        "  with torch.no_grad():\n",
        "    x_reconstructed_init = hqs_init(ytest, sigma_test)\n",
        "    x_reconstructed_single = hqs_opt_single(ytest, sigma_test)\n",
        "    x_reconstructed_train = hqs_trained_dataset(ytest, sigma_test)\n",
        "    dinv.utils.plot([xtest,ytest,x_reconstructed_init,x_reconstructed_single,x_reconstructed_train], ['Original', 'Degraded  (%.2f dB)'%psnr(xtest,ytest),'Rec (Init) (%.2f dB)'%psnr(xtest,x_reconstructed_init),'Rec (Single) (%.2f dB)'%psnr(xtest,x_reconstructed_single),'Rec (Dataset) (%.2f dB)'%psnr(xtest,x_reconstructed_train)],figsize=(14,14))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}